{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "id": "lbJF_3uvzktD",
    "ExecuteTime": {
     "end_time": "2023-06-02T16:35:12.965829800Z",
     "start_time": "2023-06-02T16:35:12.916714100Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#from pytorch_lightning.callbacks import TQDMProgressBar\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from models.transformer.transformer_main import MyTransformer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from ray import tune\n",
    "from ray.tune.integration.pytorch_lightning import TuneReportCheckpointCallback\n",
    "\n",
    "from models.transformer.my_functrions import make_dataset, get_torch_data_loaders, RMSELoss\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "torch.__version__"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "TVb9TF7D3W7i",
    "outputId": "0c7134f0-fe3b-4807-ccb8-770c7ff8487c",
    "ExecuteTime": {
     "end_time": "2023-06-02T16:35:26.981809800Z",
     "start_time": "2023-06-02T16:35:26.917188400Z"
    }
   },
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "'1.13.1+cu117'"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cpu')"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n",
    "\n"
   ],
   "metadata": {
    "id": "8n53CTpUzktF",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "0d5173d2-0479-4bc2-e8d7-829102ee3441",
    "ExecuteTime": {
     "end_time": "2023-06-02T16:35:29.126792100Z",
     "start_time": "2023-06-02T16:35:29.108748Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df = pd.read_csv(\n",
    "    '../datasets/prepared/btc/ta_corr_01_fd.csv',\n",
    "    parse_dates=True\n",
    "    )\n",
    "target_col = 'log_returns'\n",
    "df = df.set_index(['timestamp'])\n",
    "df.index = pd.to_datetime(df.index)\n",
    "exclude_cols = ['log_close', 'Close']\n",
    "df"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 692
    },
    "id": "L42A3xBvmr7L",
    "outputId": "050dd1ab-4a57-4ceb-c3f8-f6cc049af895",
    "ExecuteTime": {
     "end_time": "2023-06-02T16:35:36.092472300Z",
     "start_time": "2023-06-02T16:35:35.975692100Z"
    }
   },
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "            log_close         Close  log_returns  diff_fear_greed   \ntimestamp                                                           \n2014-11-27   5.912611    369.670013     0.003523        -0.006765  \\\n2014-11-28   5.930777    376.446991     0.018166         0.007878   \n2014-11-29   5.928234    375.490997    -0.002543        -0.012831   \n2014-11-30   5.935019    378.046997     0.006784        -0.003504   \n2014-12-01   5.938182    379.244995     0.003164        -0.007124   \n...               ...           ...          ...              ...   \n2023-05-29  10.230843  27745.884766    -0.012171         0.987829   \n2023-05-30  10.229273  27702.349609    -0.001570        -0.001570   \n2023-05-31  10.211695  27219.658203    -0.017578        -1.017578   \n2023-06-01  10.196902  26819.972656    -0.014793         1.985207   \n2023-06-02  10.202378  26967.240234     0.005476        -0.004812   \n\n            volatility_kcp    volume_em    volume_vpt  momentum_stoch_rsi   \ntimestamp                                                                   \n2014-11-27        0.501327   -40.468161 -3.149466e+05            0.388997  \\\n2014-11-28        0.726950   -52.121651  4.515396e+05            0.748693   \n2014-11-29        0.700354   927.476194  3.816202e+05            0.714180   \n2014-11-30        0.764513  -196.001510  2.354075e+04            0.813134   \n2014-12-01        0.718714   133.611388  9.986327e+04            1.000000   \n...                    ...          ...           ...                 ...   \n2023-05-29        1.050246  2858.497078  4.753301e+08            0.811280   \n2023-05-30        0.955887  -624.340933 -2.044453e+08            0.787049   \n2023-05-31        0.580642 -2882.568661 -2.935917e+08            0.526399   \n2023-06-01        0.308392 -1562.736024 -4.883416e+08            0.331149   \n2023-06-02        0.421000  -499.111479 -1.314901e+08            0.417980   \n\n            volatility_bbp  momentum_stoch  ...  trend_macd_diff  trend_stc   \ntimestamp                                   ...                               \n2014-11-27        0.425509       34.013853  ...        -0.855557  24.373770  \\\n2014-11-28        0.490535       48.927911  ...        -0.354460  46.507056   \n2014-11-29        0.469128       47.481343  ...        -0.099079  68.204734   \n2014-11-30        0.496233       51.348971  ...         0.213258  84.102367   \n2014-12-01        0.504775       69.447910  ...         0.457770  92.051184   \n...                    ...             ...  ...              ...        ...   \n2023-05-29        0.932253       73.001414  ...       142.801589  75.000019   \n2023-05-30        0.897492       71.288406  ...       158.821013  87.500010   \n2023-05-31        0.601630       52.295615  ...       132.467667  93.750005   \n2023-06-01        0.362828       36.568912  ...        85.932695  96.875002   \n2023-06-02        0.444708       42.363551  ...        63.897124  98.437501   \n\n            volume_sma_em  trend_aroon_ind  momentum_tsi  trend_aroon_down   \ntimestamp                                                                    \n2014-11-27    -502.853674             36.0     -0.527280               4.0  \\\n2014-11-28    -281.225092             32.0      0.152113               4.0   \n2014-11-29       0.631289             28.0      0.610003               4.0   \n2014-11-30      40.733570             20.0      1.277897               8.0   \n2014-12-01     -12.443357             20.0      1.977937               4.0   \n...                   ...              ...           ...               ...   \n2023-05-29     451.343496            -76.0     -8.043034              80.0   \n2023-05-30     434.361789            -72.0     -5.906286              76.0   \n2023-05-31     251.374776            -68.0     -5.567434              72.0   \n2023-06-01     185.248022             16.0     -6.418554              68.0   \n2023-06-02     156.772250             16.0     -6.626771              64.0   \n\n            trend_vortex_ind_neg  trend_psar_down_indicator  trend_adx_neg   \ntimestamp                                                                    \n2014-11-27              1.288202                        0.0      20.285280  \\\n2014-11-28              1.133492                        0.0      22.078843   \n2014-11-29              1.070125                        0.0      20.806231   \n2014-11-30              1.032653                        0.0      20.063468   \n2014-12-01              1.119928                        0.0      19.494893   \n...                          ...                        ...            ...   \n2023-05-29              0.918640                        0.0      18.152938   \n2023-05-30              0.923086                        0.0      17.383497   \n2023-05-31              0.962587                        0.0      22.440350   \n2023-06-01              1.012652                        0.0      22.840257   \n2023-06-02              0.992665                        0.0      22.416712   \n\n            volatility_bbli  \ntimestamp                    \n2014-11-27              0.0  \n2014-11-28              0.0  \n2014-11-29              0.0  \n2014-11-30              0.0  \n2014-12-01              0.0  \n...                     ...  \n2023-05-29              0.0  \n2023-05-30              0.0  \n2023-05-31              0.0  \n2023-06-01              0.0  \n2023-06-02              0.0  \n\n[3110 rows x 38 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>log_close</th>\n      <th>Close</th>\n      <th>log_returns</th>\n      <th>diff_fear_greed</th>\n      <th>volatility_kcp</th>\n      <th>volume_em</th>\n      <th>volume_vpt</th>\n      <th>momentum_stoch_rsi</th>\n      <th>volatility_bbp</th>\n      <th>momentum_stoch</th>\n      <th>...</th>\n      <th>trend_macd_diff</th>\n      <th>trend_stc</th>\n      <th>volume_sma_em</th>\n      <th>trend_aroon_ind</th>\n      <th>momentum_tsi</th>\n      <th>trend_aroon_down</th>\n      <th>trend_vortex_ind_neg</th>\n      <th>trend_psar_down_indicator</th>\n      <th>trend_adx_neg</th>\n      <th>volatility_bbli</th>\n    </tr>\n    <tr>\n      <th>timestamp</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2014-11-27</th>\n      <td>5.912611</td>\n      <td>369.670013</td>\n      <td>0.003523</td>\n      <td>-0.006765</td>\n      <td>0.501327</td>\n      <td>-40.468161</td>\n      <td>-3.149466e+05</td>\n      <td>0.388997</td>\n      <td>0.425509</td>\n      <td>34.013853</td>\n      <td>...</td>\n      <td>-0.855557</td>\n      <td>24.373770</td>\n      <td>-502.853674</td>\n      <td>36.0</td>\n      <td>-0.527280</td>\n      <td>4.0</td>\n      <td>1.288202</td>\n      <td>0.0</td>\n      <td>20.285280</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2014-11-28</th>\n      <td>5.930777</td>\n      <td>376.446991</td>\n      <td>0.018166</td>\n      <td>0.007878</td>\n      <td>0.726950</td>\n      <td>-52.121651</td>\n      <td>4.515396e+05</td>\n      <td>0.748693</td>\n      <td>0.490535</td>\n      <td>48.927911</td>\n      <td>...</td>\n      <td>-0.354460</td>\n      <td>46.507056</td>\n      <td>-281.225092</td>\n      <td>32.0</td>\n      <td>0.152113</td>\n      <td>4.0</td>\n      <td>1.133492</td>\n      <td>0.0</td>\n      <td>22.078843</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2014-11-29</th>\n      <td>5.928234</td>\n      <td>375.490997</td>\n      <td>-0.002543</td>\n      <td>-0.012831</td>\n      <td>0.700354</td>\n      <td>927.476194</td>\n      <td>3.816202e+05</td>\n      <td>0.714180</td>\n      <td>0.469128</td>\n      <td>47.481343</td>\n      <td>...</td>\n      <td>-0.099079</td>\n      <td>68.204734</td>\n      <td>0.631289</td>\n      <td>28.0</td>\n      <td>0.610003</td>\n      <td>4.0</td>\n      <td>1.070125</td>\n      <td>0.0</td>\n      <td>20.806231</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2014-11-30</th>\n      <td>5.935019</td>\n      <td>378.046997</td>\n      <td>0.006784</td>\n      <td>-0.003504</td>\n      <td>0.764513</td>\n      <td>-196.001510</td>\n      <td>2.354075e+04</td>\n      <td>0.813134</td>\n      <td>0.496233</td>\n      <td>51.348971</td>\n      <td>...</td>\n      <td>0.213258</td>\n      <td>84.102367</td>\n      <td>40.733570</td>\n      <td>20.0</td>\n      <td>1.277897</td>\n      <td>8.0</td>\n      <td>1.032653</td>\n      <td>0.0</td>\n      <td>20.063468</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2014-12-01</th>\n      <td>5.938182</td>\n      <td>379.244995</td>\n      <td>0.003164</td>\n      <td>-0.007124</td>\n      <td>0.718714</td>\n      <td>133.611388</td>\n      <td>9.986327e+04</td>\n      <td>1.000000</td>\n      <td>0.504775</td>\n      <td>69.447910</td>\n      <td>...</td>\n      <td>0.457770</td>\n      <td>92.051184</td>\n      <td>-12.443357</td>\n      <td>20.0</td>\n      <td>1.977937</td>\n      <td>4.0</td>\n      <td>1.119928</td>\n      <td>0.0</td>\n      <td>19.494893</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2023-05-29</th>\n      <td>10.230843</td>\n      <td>27745.884766</td>\n      <td>-0.012171</td>\n      <td>0.987829</td>\n      <td>1.050246</td>\n      <td>2858.497078</td>\n      <td>4.753301e+08</td>\n      <td>0.811280</td>\n      <td>0.932253</td>\n      <td>73.001414</td>\n      <td>...</td>\n      <td>142.801589</td>\n      <td>75.000019</td>\n      <td>451.343496</td>\n      <td>-76.0</td>\n      <td>-8.043034</td>\n      <td>80.0</td>\n      <td>0.918640</td>\n      <td>0.0</td>\n      <td>18.152938</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2023-05-30</th>\n      <td>10.229273</td>\n      <td>27702.349609</td>\n      <td>-0.001570</td>\n      <td>-0.001570</td>\n      <td>0.955887</td>\n      <td>-624.340933</td>\n      <td>-2.044453e+08</td>\n      <td>0.787049</td>\n      <td>0.897492</td>\n      <td>71.288406</td>\n      <td>...</td>\n      <td>158.821013</td>\n      <td>87.500010</td>\n      <td>434.361789</td>\n      <td>-72.0</td>\n      <td>-5.906286</td>\n      <td>76.0</td>\n      <td>0.923086</td>\n      <td>0.0</td>\n      <td>17.383497</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2023-05-31</th>\n      <td>10.211695</td>\n      <td>27219.658203</td>\n      <td>-0.017578</td>\n      <td>-1.017578</td>\n      <td>0.580642</td>\n      <td>-2882.568661</td>\n      <td>-2.935917e+08</td>\n      <td>0.526399</td>\n      <td>0.601630</td>\n      <td>52.295615</td>\n      <td>...</td>\n      <td>132.467667</td>\n      <td>93.750005</td>\n      <td>251.374776</td>\n      <td>-68.0</td>\n      <td>-5.567434</td>\n      <td>72.0</td>\n      <td>0.962587</td>\n      <td>0.0</td>\n      <td>22.440350</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2023-06-01</th>\n      <td>10.196902</td>\n      <td>26819.972656</td>\n      <td>-0.014793</td>\n      <td>1.985207</td>\n      <td>0.308392</td>\n      <td>-1562.736024</td>\n      <td>-4.883416e+08</td>\n      <td>0.331149</td>\n      <td>0.362828</td>\n      <td>36.568912</td>\n      <td>...</td>\n      <td>85.932695</td>\n      <td>96.875002</td>\n      <td>185.248022</td>\n      <td>16.0</td>\n      <td>-6.418554</td>\n      <td>68.0</td>\n      <td>1.012652</td>\n      <td>0.0</td>\n      <td>22.840257</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2023-06-02</th>\n      <td>10.202378</td>\n      <td>26967.240234</td>\n      <td>0.005476</td>\n      <td>-0.004812</td>\n      <td>0.421000</td>\n      <td>-499.111479</td>\n      <td>-1.314901e+08</td>\n      <td>0.417980</td>\n      <td>0.444708</td>\n      <td>42.363551</td>\n      <td>...</td>\n      <td>63.897124</td>\n      <td>98.437501</td>\n      <td>156.772250</td>\n      <td>16.0</td>\n      <td>-6.626771</td>\n      <td>64.0</td>\n      <td>0.992665</td>\n      <td>0.0</td>\n      <td>22.416712</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>3110 rows × 38 columns</p>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# config = {\n",
    "#     'batch_size': tune.grid_search([4]),\n",
    "#     'num_layers': tune.grid_search([4]),\n",
    "#     'dropout': tune.grid_search([0.2]),\n",
    "#     'nhead': tune.grid_search([1]),\n",
    "#     #'attn_type': tune.grid_search([ 'dense', '', 'rv_mix']),\n",
    "#     #'attn_type': tune.grid_search(['', 'fac_dense', 'dense', 'rv_mix', 'dv_mix']),\n",
    "#     'attn_type': tune.grid_search(['fac_dense',]),\n",
    "#     'learning_rate': tune.grid_search([1e-5]),\n",
    "#     'weight_decay': tune.grid_search([1e-6]),\n",
    "#     'patience': tune.grid_search([50]),\n",
    "#     'n_epochs': tune.grid_search([5]),\n",
    "#     'timestep': tune.grid_search([30]),\n",
    "#     'horizon': tune.grid_search([21]),\n",
    "#     'ntest': tune.grid_search([21]),\n",
    "#     'model_checkpoint_outputdir': 'multioutput/ta_corr_01_fd2'\n",
    "# }\n",
    "config = {\n",
    "    'batch_size': tune.choice([4, 8]),\n",
    "    'num_layers': tune.choice([4, 8]),\n",
    "    'dropout': tune.choice([0.2]),\n",
    "    'nhead': tune.choice([1, 4, 6]),\n",
    "    #'attn_type': tune.grid_search([ 'dense', '', 'rv_mix']),\n",
    "    #'attn_type': tune.grid_search(['', 'fac_dense', 'dense', 'rv_mix', 'dv_mix']),\n",
    "    'attn_type': tune.choice(['fac_dense', '']),\n",
    "    'learning_rate': tune.choice([1e-5]),\n",
    "    'weight_decay': tune.choice([1e-6]),\n",
    "    'patience': tune.choice([25]),\n",
    "    'n_epochs': tune.choice([50]),\n",
    "    'timestep': tune.choice([10, 20, 30]),\n",
    "    'horizon': tune.choice([21]),\n",
    "    'ntest': tune.choice([42]),\n",
    "    'model_checkpoint_outputdir': 'multioutput/ta_corr_01_fd3'\n",
    "}\n"
   ],
   "metadata": {
    "id": "1CWxZOTqnn6E"
   },
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "def train_transformer(config):\n",
    "    print('CUDA', torch.cuda.is_available())\n",
    "    timestep= config['timestep']\n",
    "    ntest = config['ntest']\n",
    "    horizon = config['horizon']\n",
    "    decoder_size = 16\n",
    "\n",
    "    Xtrain, Ytrain, Xtest, Ytest, XVal, YVal, scaler = make_dataset(\n",
    "        df, target_col='log_returns', exclude_cols=exclude_cols, timestep=timestep, ntest=ntest, horizon=horizon\n",
    "    )\n",
    "    train_loader, val_loader, test_loader, test_loader_one = get_torch_data_loaders(\n",
    "        Xtrain, Ytrain, Xtest, Ytest, XVal, YVal, config['batch_size']\n",
    "    )\n",
    "    feature_size = Xtrain.shape[-1] #input_dim\n",
    "\n",
    "    loss_fn = RMSELoss\n",
    "\n",
    "    # loss_fn = RMSELoss\n",
    "    # # train\n",
    "    \n",
    "    model = MyTransformer(\n",
    "        loss_fn=loss_fn,\n",
    "        batch_size=config['batch_size'],\n",
    "        decoder_size=decoder_size, \n",
    "        timestep=timestep,\n",
    "        horizon=horizon,\n",
    "        feature_size=feature_size,\n",
    "        num_layers=config['num_layers'],\n",
    "        dropout=config['dropout'],\n",
    "        nhead=config['nhead'],\n",
    "        attn_type=config['attn_type'],\n",
    "        learning_rate=config['learning_rate'],\n",
    "        weight_decay=config['weight_decay']\n",
    "    )\n",
    "\n",
    "\n",
    "    early_stop_callback = EarlyStopping(monitor=\"val_loss\", patience=config['patience'], verbose=False, mode=\"min\")\n",
    "\n",
    "    filename = f\"{config['attn_type']}_timestep={timestep}\" + '{epoch}-{val_loss:.3f}'\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=f\"./checkpoints/{config['model_checkpoint_outputdir']}\",\n",
    "        filename=filename,\n",
    "        save_top_k=1, \n",
    "        monitor=\"val_loss\"\n",
    "    )\n",
    "    \n",
    "    tune_report_callback = TuneReportCheckpointCallback(\n",
    "        {\"loss\": \"val_loss\"},\n",
    "        on=\"validation_end\"\n",
    "    )\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        callbacks=[early_stop_callback,checkpoint_callback, tune_report_callback], \n",
    "        max_epochs=config['n_epochs'],\n",
    "        logger=False,\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "    #=========================================================================\n",
    "    #TODO SAVE LOGS TO GOOGLE DRIVE\n",
    "    # dont save all checkpoints!\n",
    "    # add search algrothitm!\n",
    "    #========================================================================="
   ],
   "metadata": {
    "id": "V_6vQQJUmxu-",
    "ExecuteTime": {
     "end_time": "2023-06-02T16:48:05.511331800Z",
     "start_time": "2023-06-02T16:48:05.506823100Z"
    }
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# analysis = tune.run(\n",
    "#     tune.with_parameters(train_transformer),\n",
    "#     config=config,\n",
    "#     num_samples=10,\n",
    "#     resources_per_trial={\"cpu\": 1, \"gpu\": 1},\n",
    "#  )\n",
    "\n",
    "trainable = tune.with_resources(\n",
    "    train_transformer,\n",
    "    {\n",
    "        \"cpu\": 4,\n",
    "        #\"gpu\": 1,\n",
    "        #'accelerator': NVIDIA_TESLA_A100\n",
    "    }\n",
    ")\n",
    "\n",
    "analysis = tune.run(\n",
    "    trainable,\n",
    "    # resources_per_trial={\n",
    "    #     \"cpu\": 12,\n",
    "    #     \"gpu\": 1,\n",
    "    #     'accelerator_type': NVIDIA_TESLA_A100\n",
    "    # },\n",
    "    metric=\"loss\",\n",
    "    mode=\"min\",\n",
    "    config=config,\n",
    "    #num_samples=4,\n",
    "    name=\"tune_transformer\",\n",
    "    keep_checkpoints_num=3,\n",
    "    local_dir=f\"./checkpoints/{config['model_checkpoint_outputdir']}\"\n",
    ")"
   ],
   "metadata": {
    "id": "-5XYIDsVkQzz",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "b7161495-7c29-4993-8a4c-847ee3b64e1c",
    "ExecuteTime": {
     "end_time": "2023-06-02T16:51:30.153681600Z",
     "start_time": "2023-06-02T16:48:09.553090100Z"
    }
   },
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-02 19:48:11,613\tINFO worker.py:1625 -- Started a local Ray instance.\n",
      "2023-06-02 19:48:12,449\tINFO tune.py:218 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `tune.run(...)`.\n",
      "/home/olexandr/myenvs2/diploma1/lib/python3.9/site-packages/ray/tune/experiment/experiment.py:170: UserWarning:\n",
      "\n",
      "The `local_dir` argument of `Experiment is deprecated. Use `storage_path` or set the `TUNE_RESULT_DIR` environment variable instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m CUDA False\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m X.shape (3060, 30, 36) Y.shape (3060, 21)\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m kw_batch,feature size:  4 36\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m embed_dim: 36\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m num_heads:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m GPU available: False, used: False\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m TPU available: False, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m IPU available: False, using: 0 IPUs\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m HPU available: False, using: 0 HPUs\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m \n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m   | Name                | Type                    | Params\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m ----------------------------------------------------------------\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m 0 | pos_encoder         | PositionalEncoding      | 0     \n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m 1 | encoder_layer       | TransformerEncoderLayer | 155 K \n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m 2 | transformer_encoder | TransformerEncoder      | 620 K \n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m 3 | decoder1            | Linear                  | 592   \n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m 4 | decoder2            | Linear                  | 10.1 K\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m ----------------------------------------------------------------\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m 785 K     Trainable params\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m 785 K     Total params\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m 3.143     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m /home/olexandr/myenvs2/diploma1/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning:\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m \n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m \n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m /home/olexandr/myenvs2/diploma1/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning:\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m \n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/754 [00:00<?, ?it/s]                           \n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss \n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m tensor(2.2151, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:   0%|          | 1/754 [00:00<01:01, 12.20it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(2.3022, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:   0%|          | 2/754 [00:00<01:08, 11.04it/s]training_loss tensor(2.1323, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:   0%|          | 3/754 [00:00<00:58, 12.75it/s]training_loss tensor(2.2843, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:   1%|          | 4/754 [00:00<00:53, 14.00it/s]training_loss tensor(2.1442, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:   1%|          | 5/754 [00:00<00:50, 14.81it/s]training_loss tensor(1.3571, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:   1%|          | 6/754 [00:00<00:48, 15.35it/s]training_loss tensor(1.1158, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:   1%|          | 7/754 [00:00<00:52, 14.34it/s]training_loss tensor(1.2665, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:   1%|          | 8/754 [00:00<00:51, 14.56it/s]training_loss tensor(1.2169, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:   1%|          | 9/754 [00:00<00:49, 14.99it/s]training_loss tensor(1.1407, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:   1%|▏         | 10/754 [00:00<00:48, 15.36it/s]training_loss tensor(1.0823, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:   1%|▏         | 11/754 [00:00<00:47, 15.64it/s]training_loss tensor(1.0832, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:   2%|▏         | 12/754 [00:00<00:46, 15.93it/s]training_loss tensor(1.0406, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:   2%|▏         | 13/754 [00:00<00:45, 16.18it/s]training_loss tensor(0.8340, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:   2%|▏         | 14/754 [00:00<00:45, 16.36it/s]training_loss tensor(0.8298, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:   2%|▏         | 15/754 [00:00<00:44, 16.55it/s]training_loss tensor(0.9538, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:   2%|▏         | 16/754 [00:00<00:44, 16.69it/s]training_loss tensor(1.0226, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:   2%|▏         | 17/754 [00:01<00:43, 16.88it/s]training_loss tensor(1.0675, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:   2%|▏         | 18/754 [00:01<00:43, 17.05it/s]training_loss tensor(0.9693, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:   3%|▎         | 19/754 [00:01<00:42, 17.17it/s]training_loss tensor(1.0868, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:   3%|▎         | 20/754 [00:01<00:42, 17.31it/s]training_loss tensor(0.8790, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:   3%|▎         | 21/754 [00:01<00:42, 17.41it/s]training_loss tensor(0.8079, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:   3%|▎         | 22/754 [00:01<00:41, 17.52it/s]training_loss tensor(0.7254, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:   3%|▎         | 23/754 [00:01<00:41, 17.63it/s]training_loss tensor(0.7474, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:   3%|▎         | 24/754 [00:01<00:41, 17.74it/s]training_loss tensor(0.6701, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:   3%|▎         | 25/754 [00:01<00:40, 17.80it/s]training_loss tensor(0.6958, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:   3%|▎         | 26/754 [00:01<00:40, 17.90it/s]\n",
      "Epoch 0:   3%|▎         | 26/754 [00:01<00:40, 17.89it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.7318, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:   4%|▎         | 27/754 [00:01<00:40, 17.95it/s]training_loss tensor(0.7140, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:   4%|▎         | 28/754 [00:01<00:40, 18.03it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.6502, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:   4%|▍         | 29/754 [00:01<00:40, 18.08it/s]training_loss tensor(0.7450, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:   4%|▍         | 30/754 [00:01<00:39, 18.14it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.6907, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:   4%|▍         | 31/754 [00:01<00:39, 18.13it/s]training_loss tensor(0.5489, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:   4%|▍         | 32/754 [00:01<00:39, 18.15it/s]training_loss tensor(0.5203, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:   4%|▍         | 33/754 [00:01<00:39, 18.19it/s]training_loss tensor(0.3817, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:   5%|▍         | 34/754 [00:01<00:39, 18.24it/s]training_loss tensor(0.5274, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:   5%|▍         | 35/754 [00:01<00:39, 18.26it/s]training_loss tensor(0.4448, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:   5%|▍         | 36/754 [00:01<00:39, 18.30it/s]training_loss tensor(0.5027, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:   5%|▍         | 37/754 [00:02<00:39, 18.35it/s]training_loss tensor(0.4463, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:   5%|▌         | 38/754 [00:02<00:38, 18.40it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.5228, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:   5%|▌         | 39/754 [00:02<00:38, 18.42it/s]training_loss tensor(0.5611, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:   5%|▌         | 40/754 [00:02<00:38, 18.47it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.5846, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:   5%|▌         | 41/754 [00:02<00:38, 18.50it/s]training_loss tensor(0.5444, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:   6%|▌         | 42/754 [00:02<00:38, 18.53it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.5935, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:   6%|▌         | 43/754 [00:02<00:38, 18.56it/s]training_loss tensor(0.5559, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:   6%|▌         | 44/754 [00:02<00:38, 18.60it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.6839, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:   6%|▌         | 45/754 [00:02<00:38, 18.61it/s]training_loss tensor(0.7915, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:   6%|▌         | 46/754 [00:02<00:37, 18.64it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.7896, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:   6%|▌         | 47/754 [00:02<00:37, 18.65it/s]training_loss tensor(0.8761, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:   6%|▋         | 48/754 [00:02<00:37, 18.69it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.7991, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:   6%|▋         | 49/754 [00:02<00:37, 18.71it/s]training_loss tensor(0.6895, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:   7%|▋         | 50/754 [00:02<00:37, 18.72it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.5592, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:   7%|▋         | 51/754 [00:02<00:37, 18.71it/s]training_loss tensor(0.6468, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:   7%|▋         | 52/754 [00:02<00:37, 18.72it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.5967, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:   7%|▋         | 53/754 [00:02<00:37, 18.73it/s]training_loss tensor(1.0387, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:   7%|▋         | 54/754 [00:02<00:37, 18.73it/s]training_loss tensor(1.4419, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:   7%|▋         | 55/754 [00:02<00:37, 18.74it/s]training_loss tensor(1.5321, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:   7%|▋         | 56/754 [00:02<00:37, 18.74it/s]training_loss tensor(1.4432, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:   8%|▊         | 57/754 [00:03<00:37, 18.75it/s]training_loss tensor(1.4231, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:   8%|▊         | 58/754 [00:03<00:37, 18.75it/s]training_loss tensor(1.2533, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:   8%|▊         | 59/754 [00:03<00:37, 18.75it/s]training_loss tensor(0.7173, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:   8%|▊         | 60/754 [00:03<00:37, 18.75it/s]training_loss tensor(0.6332, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:   8%|▊         | 61/754 [00:03<00:36, 18.76it/s]training_loss tensor(0.5840, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:   8%|▊         | 62/754 [00:03<00:36, 18.77it/s]training_loss tensor(0.5321, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:   8%|▊         | 63/754 [00:03<00:36, 18.78it/s]training_loss tensor(0.4555, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:   8%|▊         | 64/754 [00:03<00:36, 18.79it/s]training_loss tensor(0.4801, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:   9%|▊         | 65/754 [00:03<00:36, 18.81it/s]training_loss tensor(0.4193, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:   9%|▉         | 66/754 [00:03<00:36, 18.83it/s]training_loss tensor(0.4219, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:   9%|▉         | 67/754 [00:03<00:36, 18.85it/s]training_loss tensor(0.4428, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:   9%|▉         | 68/754 [00:03<00:36, 18.87it/s]training_loss tensor(0.4254, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:   9%|▉         | 69/754 [00:03<00:36, 18.88it/s]training_loss tensor(0.4868, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:   9%|▉         | 70/754 [00:03<00:36, 18.89it/s]training_loss tensor(0.5120, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:   9%|▉         | 71/754 [00:03<00:36, 18.87it/s]training_loss tensor(0.5645, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  10%|▉         | 72/754 [00:03<00:36, 18.88it/s]training_loss tensor(0.8998, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  10%|▉         | 73/754 [00:03<00:36, 18.90it/s]training_loss tensor(1.1538, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  10%|▉         | 74/754 [00:03<00:35, 18.92it/s]training_loss tensor(1.2994, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  10%|▉         | 75/754 [00:03<00:35, 18.91it/s]training_loss tensor(1.4758, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  10%|█         | 76/754 [00:04<00:35, 18.92it/s]training_loss tensor(1.5180, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  10%|█         | 77/754 [00:04<00:35, 18.93it/s]training_loss tensor(1.5130, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  10%|█         | 78/754 [00:04<00:35, 18.94it/s]training_loss tensor(1.2055, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  10%|█         | 79/754 [00:04<00:35, 18.95it/s]training_loss tensor(1.1171, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  11%|█         | 80/754 [00:04<00:35, 18.92it/s]training_loss tensor(0.8103, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  11%|█         | 81/754 [00:04<00:35, 18.89it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.7913, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  11%|█         | 82/754 [00:04<00:35, 18.88it/s]training_loss tensor(0.8922, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  11%|█         | 83/754 [00:04<00:35, 18.88it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.9924, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  11%|█         | 84/754 [00:04<00:35, 18.88it/s]training_loss tensor(0.9336, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  11%|█▏        | 85/754 [00:04<00:35, 18.89it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.9051, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  11%|█▏        | 86/754 [00:04<00:35, 18.90it/s]training_loss tensor(0.9571, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  12%|█▏        | 87/754 [00:04<00:35, 18.88it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.8452, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  12%|█▏        | 88/754 [00:04<00:35, 18.88it/s]training_loss tensor(0.7279, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  12%|█▏        | 89/754 [00:04<00:35, 18.89it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.8120, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  12%|█▏        | 90/754 [00:04<00:35, 18.90it/s]training_loss tensor(0.7797, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  12%|█▏        | 91/754 [00:04<00:35, 18.91it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.1314, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  12%|█▏        | 92/754 [00:04<00:34, 18.92it/s]training_loss tensor(1.2559, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  12%|█▏        | 93/754 [00:04<00:34, 18.92it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.3937, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  12%|█▏        | 94/754 [00:04<00:34, 18.91it/s]training_loss tensor(1.3394, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  13%|█▎        | 95/754 [00:05<00:34, 18.92it/s]training_loss tensor(1.3526, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  13%|█▎        | 96/754 [00:05<00:34, 18.92it/s]training_loss tensor(1.0851, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  13%|█▎        | 97/754 [00:05<00:34, 18.94it/s]training_loss tensor(0.8183, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  13%|█▎        | 98/754 [00:05<00:34, 18.95it/s]training_loss tensor(0.5954, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  13%|█▎        | 99/754 [00:05<00:34, 18.96it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.5990, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  13%|█▎        | 100/754 [00:05<00:34, 18.95it/s]training_loss tensor(0.6183, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  13%|█▎        | 101/754 [00:05<00:34, 18.95it/s]training_loss tensor(0.5965, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  14%|█▎        | 102/754 [00:05<00:34, 18.95it/s]training_loss tensor(0.5859, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  14%|█▎        | 103/754 [00:05<00:34, 18.95it/s]training_loss tensor(0.6067, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  14%|█▍        | 104/754 [00:05<00:34, 18.94it/s]training_loss tensor(0.5241, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  14%|█▍        | 105/754 [00:05<00:34, 18.92it/s]training_loss tensor(0.5036, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  14%|█▍        | 106/754 [00:05<00:34, 18.92it/s]training_loss tensor(0.3968, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  14%|█▍        | 107/754 [00:05<00:34, 18.92it/s]training_loss tensor(0.4836, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  14%|█▍        | 108/754 [00:05<00:34, 18.93it/s]training_loss tensor(0.4228, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  14%|█▍        | 109/754 [00:05<00:34, 18.92it/s]training_loss tensor(0.4422, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  15%|█▍        | 110/754 [00:05<00:34, 18.93it/s]training_loss tensor(0.4016, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  15%|█▍        | 111/754 [00:05<00:33, 18.94it/s]training_loss tensor(0.3983, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  15%|█▍        | 112/754 [00:05<00:33, 18.94it/s]training_loss \n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m tensor(0.3815, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  15%|█▍        | 113/754 [00:05<00:33, 18.95it/s]training_loss tensor(0.2884, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  15%|█▌        | 114/754 [00:06<00:33, 18.96it/s]training_loss tensor(0.2712, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  15%|█▌        | 115/754 [00:06<00:33, 18.97it/s]training_loss tensor(0.3145, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  15%|█▌        | 116/754 [00:06<00:33, 18.97it/s]training_loss tensor(0.3936, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  16%|█▌        | 117/754 [00:06<00:33, 18.98it/s]training_loss tensor(0.4442, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  16%|█▌        | 118/754 [00:06<00:33, 18.99it/s]training_loss tensor(0.4768, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  16%|█▌        | 119/754 [00:06<00:33, 18.99it/s]training_loss tensor(0.5123, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  16%|█▌        | 120/754 [00:06<00:33, 19.00it/s]training_loss tensor(0.5095, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  16%|█▌        | 121/754 [00:06<00:33, 19.01it/s]training_loss tensor(0.4546, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  16%|█▌        | 122/754 [00:06<00:33, 19.02it/s]training_loss tensor(0.3950, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  16%|█▋        | 123/754 [00:06<00:33, 19.04it/s]training_loss tensor(0.4341, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  16%|█▋        | 124/754 [00:06<00:33, 19.05it/s]training_loss tensor(0.6126, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  17%|█▋        | 125/754 [00:06<00:33, 19.06it/s]training_loss tensor(0.7639, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  17%|█▋        | 126/754 [00:06<00:32, 19.06it/s]training_loss tensor(0.8192, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  17%|█▋        | 127/754 [00:06<00:32, 19.06it/s]training_loss tensor(0.8249, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  17%|█▋        | 128/754 [00:06<00:32, 19.06it/s]training_loss tensor(1.0418, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  17%|█▋        | 129/754 [00:06<00:32, 19.06it/s]training_loss tensor(1.1132, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  17%|█▋        | 130/754 [00:06<00:32, 19.07it/s]training_loss tensor(1.1458, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  17%|█▋        | 131/754 [00:06<00:32, 19.08it/s]training_loss tensor(1.4098, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  18%|█▊        | 132/754 [00:06<00:32, 19.09it/s]training_loss tensor(1.4777, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  18%|█▊        | 133/754 [00:06<00:32, 19.09it/s]training_loss tensor(1.3869, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  18%|█▊        | 134/754 [00:07<00:32, 19.10it/s]training_loss tensor(1.3729, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  18%|█▊        | 135/754 [00:07<00:32, 19.11it/s]training_loss tensor(1.2626, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  18%|█▊        | 136/754 [00:07<00:32, 19.12it/s]training_loss tensor(0.9377, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  18%|█▊        | 137/754 [00:07<00:32, 19.13it/s]training_loss tensor(0.8159, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  18%|█▊        | 138/754 [00:07<00:32, 19.13it/s]training_loss tensor(0.7188, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  18%|█▊        | 139/754 [00:07<00:32, 19.13it/s]training_loss tensor(0.5363, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  19%|█▊        | 140/754 [00:07<00:32, 19.14it/s]training_loss tensor(0.4629, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  19%|█▊        | 141/754 [00:07<00:32, 19.14it/s]training_loss tensor(0.7941, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  19%|█▉        | 142/754 [00:07<00:31, 19.14it/s]training_loss tensor(0.7902, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  19%|█▉        | 143/754 [00:07<00:31, 19.14it/s]training_loss tensor(0.7817, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  19%|█▉        | 144/754 [00:07<00:31, 19.13it/s]training_loss tensor(0.7966, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  19%|█▉        | 145/754 [00:07<00:31, 19.13it/s]training_loss tensor(0.7790, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  19%|█▉        | 146/754 [00:07<00:31, 19.13it/s]training_loss tensor(0.4828, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  19%|█▉        | 147/754 [00:07<00:31, 19.13it/s]training_loss tensor(0.3523, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  20%|█▉        | 148/754 [00:07<00:31, 19.14it/s]training_loss tensor(0.3847, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  20%|█▉        | 149/754 [00:07<00:31, 19.15it/s]training_loss tensor(0.3863, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  20%|█▉        | 150/754 [00:07<00:31, 19.15it/s]training_loss tensor(0.3966, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  20%|██        | 151/754 [00:07<00:31, 19.16it/s]training_loss tensor(0.4551, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  20%|██        | 152/754 [00:07<00:31, 19.16it/s]training_loss tensor(0.4769, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  20%|██        | 153/754 [00:07<00:31, 19.17it/s]training_loss tensor(0.4037, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  20%|██        | 154/754 [00:08<00:31, 19.17it/s]training_loss tensor(0.3171, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  21%|██        | 155/754 [00:08<00:31, 19.17it/s]training_loss tensor(0.3281, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  21%|██        | 156/754 [00:08<00:31, 19.17it/s]training_loss tensor(0.2801, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  21%|██        | 157/754 [00:08<00:31, 19.18it/s]training_loss tensor(0.3483, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  21%|██        | 158/754 [00:08<00:31, 19.18it/s]training_loss tensor(0.3136, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  21%|██        | 159/754 [00:08<00:31, 19.19it/s]training_loss tensor(0.3418, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  21%|██        | 160/754 [00:08<00:30, 19.19it/s]training_loss tensor(0.3437, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  21%|██▏       | 161/754 [00:08<00:30, 19.20it/s]training_loss tensor(0.3795, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  21%|██▏       | 162/754 [00:08<00:30, 19.21it/s]training_loss tensor(0.4222, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  22%|██▏       | 163/754 [00:08<00:30, 19.21it/s]training_loss tensor(0.4665, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  22%|██▏       | 164/754 [00:08<00:30, 19.21it/s]training_loss tensor(0.6428, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  22%|██▏       | 165/754 [00:08<00:30, 19.22it/s]training_loss tensor(0.6758, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  22%|██▏       | 166/754 [00:08<00:30, 19.23it/s]training_loss tensor(0.6777, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  22%|██▏       | 167/754 [00:08<00:30, 19.23it/s]training_loss tensor(0.6484, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  22%|██▏       | 168/754 [00:08<00:30, 19.24it/s]training_loss tensor(0.6714, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  22%|██▏       | 169/754 [00:08<00:30, 19.24it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.5563, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  23%|██▎       | 170/754 [00:08<00:30, 19.25it/s]training_loss tensor(0.4396, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  23%|██▎       | 171/754 [00:08<00:30, 19.25it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.4225, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  23%|██▎       | 172/754 [00:08<00:30, 19.26it/s]training_loss tensor(0.4523, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  23%|██▎       | 173/754 [00:08<00:30, 19.26it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.3816, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  23%|██▎       | 174/754 [00:09<00:30, 19.27it/s]training_loss tensor(0.3514, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  23%|██▎       | 175/754 [00:09<00:30, 19.27it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.3253, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  23%|██▎       | 176/754 [00:09<00:29, 19.27it/s]training_loss tensor(0.4519, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  23%|██▎       | 177/754 [00:09<00:29, 19.28it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.5624, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  24%|██▎       | 178/754 [00:09<00:29, 19.27it/s]training_loss tensor(0.6079, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  24%|██▎       | 179/754 [00:09<00:29, 19.27it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.6921, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  24%|██▍       | 180/754 [00:09<00:29, 19.27it/s]training_loss tensor(1.2839, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  24%|██▍       | 181/754 [00:09<00:29, 19.27it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.4875, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  24%|██▍       | 182/754 [00:09<00:29, 19.27it/s]training_loss tensor(1.5685, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  24%|██▍       | 183/754 [00:09<00:29, 19.26it/s]training_loss tensor(1.5869, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  24%|██▍       | 184/754 [00:09<00:29, 19.25it/s]training_loss tensor(1.5927, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  25%|██▍       | 185/754 [00:09<00:29, 19.24it/s]training_loss tensor(1.2747, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  25%|██▍       | 186/754 [00:09<00:29, 19.24it/s]training_loss tensor(1.0295, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  25%|██▍       | 187/754 [00:09<00:29, 19.23it/s]training_loss tensor(0.7246, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  25%|██▍       | 188/754 [00:09<00:29, 19.22it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.5942, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  25%|██▌       | 189/754 [00:09<00:29, 19.20it/s]training_loss tensor(0.6649, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  25%|██▌       | 190/754 [00:09<00:29, 19.20it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.6126, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  25%|██▌       | 191/754 [00:09<00:29, 19.20it/s]training_loss tensor(0.6123, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  25%|██▌       | 192/754 [00:10<00:29, 19.20it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.6015, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  26%|██▌       | 193/754 [00:10<00:29, 19.20it/s]training_loss tensor(0.6659, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  26%|██▌       | 194/754 [00:10<00:29, 19.20it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.5758, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  26%|██▌       | 195/754 [00:10<00:29, 19.20it/s]training_loss tensor(0.6478, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  26%|██▌       | 196/754 [00:10<00:29, 19.19it/s]training_loss tensor(0.8003, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  26%|██▌       | 197/754 [00:10<00:29, 19.19it/s]training_loss tensor(0.8597, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  26%|██▋       | 198/754 [00:10<00:28, 19.18it/s]training_loss tensor(1.2717, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  26%|██▋       | 199/754 [00:10<00:28, 19.17it/s]training_loss tensor(1.3774, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  27%|██▋       | 200/754 [00:10<00:28, 19.17it/s]training_loss tensor(1.4739, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  27%|██▋       | 201/754 [00:10<00:28, 19.17it/s]training_loss tensor(1.4517, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  27%|██▋       | 202/754 [00:10<00:28, 19.17it/s]training_loss tensor(1.3754, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  27%|██▋       | 203/754 [00:10<00:28, 19.16it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.1425, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  27%|██▋       | 204/754 [00:10<00:28, 19.16it/s]training_loss tensor(0.8910, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  27%|██▋       | 205/754 [00:10<00:28, 19.16it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.6732, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  27%|██▋       | 206/754 [00:10<00:28, 19.16it/s]training_loss tensor(0.5171, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  27%|██▋       | 207/754 [00:10<00:28, 19.17it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.4912, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  28%|██▊       | 208/754 [00:10<00:28, 19.17it/s]training_loss tensor(0.4550, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  28%|██▊       | 209/754 [00:10<00:28, 19.17it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.4772, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  28%|██▊       | 210/754 [00:10<00:28, 19.17it/s]training_loss tensor(0.5941, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  28%|██▊       | 211/754 [00:11<00:28, 19.17it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.7471, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  28%|██▊       | 212/754 [00:11<00:28, 19.17it/s]training_loss tensor(0.9006, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  28%|██▊       | 213/754 [00:11<00:28, 19.17it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.9144, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  28%|██▊       | 214/754 [00:11<00:28, 19.16it/s]training_loss tensor(1.0362, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  29%|██▊       | 215/754 [00:11<00:28, 19.16it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.2408, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  29%|██▊       | 216/754 [00:11<00:28, 19.16it/s]training_loss tensor(1.2619, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  29%|██▉       | 217/754 [00:11<00:28, 19.16it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.1970, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  29%|██▉       | 218/754 [00:11<00:27, 19.16it/s]training_loss tensor(1.3026, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  29%|██▉       | 219/754 [00:11<00:27, 19.16it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.3279, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  29%|██▉       | 220/754 [00:11<00:27, 19.16it/s]training_loss tensor(1.2850, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  29%|██▉       | 221/754 [00:11<00:27, 19.16it/s]training_loss tensor(1.2616, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  29%|██▉       | 222/754 [00:11<00:27, 19.16it/s]training_loss tensor(1.1872, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  30%|██▉       | 223/754 [00:11<00:27, 19.16it/s]training_loss tensor(1.1738, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  30%|██▉       | 224/754 [00:11<00:27, 19.15it/s]training_loss tensor(1.0310, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  30%|██▉       | 225/754 [00:11<00:27, 19.14it/s]training_loss tensor(0.8119, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  30%|██▉       | 226/754 [00:11<00:27, 19.14it/s]training_loss tensor(0.7372, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  30%|███       | 227/754 [00:11<00:27, 19.14it/s]training_loss tensor(0.8306, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  30%|███       | 228/754 [00:11<00:27, 19.15it/s]training_loss tensor(1.1974, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  30%|███       | 229/754 [00:11<00:27, 19.15it/s]training_loss tensor(1.7943, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  31%|███       | 230/754 [00:12<00:27, 19.15it/s]training_loss tensor(1.8084, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  31%|███       | 231/754 [00:12<00:27, 19.16it/s]training_loss tensor(1.8599, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  31%|███       | 232/754 [00:12<00:27, 19.16it/s]training_loss tensor(1.9380, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  31%|███       | 233/754 [00:12<00:27, 19.17it/s]training_loss tensor(1.7967, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  31%|███       | 234/754 [00:12<00:27, 19.17it/s]training_loss tensor(1.3320, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  31%|███       | 235/754 [00:12<00:27, 19.17it/s]training_loss tensor(1.2619, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  31%|███▏      | 236/754 [00:12<00:27, 19.17it/s]training_loss tensor(1.1941, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  31%|███▏      | 237/754 [00:12<00:26, 19.18it/s]training_loss tensor(1.1312, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  32%|███▏      | 238/754 [00:12<00:26, 19.18it/s]training_loss tensor(1.0256, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  32%|███▏      | 239/754 [00:12<00:26, 19.18it/s]training_loss tensor(0.8664, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  32%|███▏      | 240/754 [00:12<00:26, 19.18it/s]training_loss tensor(0.8893, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  32%|███▏      | 241/754 [00:12<00:26, 19.19it/s]training_loss tensor(0.9548, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  32%|███▏      | 242/754 [00:12<00:26, 19.19it/s]training_loss tensor(1.0332, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  32%|███▏      | 243/754 [00:12<00:26, 19.19it/s]training_loss tensor(1.6913, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  32%|███▏      | 244/754 [00:12<00:26, 19.20it/s]training_loss tensor(1.9511, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  32%|███▏      | 245/754 [00:12<00:26, 19.20it/s]training_loss tensor(1.9143, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  33%|███▎      | 246/754 [00:12<00:26, 19.20it/s]training_loss tensor(1.8688, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  33%|███▎      | 247/754 [00:12<00:26, 19.20it/s]training_loss tensor(1.8383, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  33%|███▎      | 248/754 [00:12<00:26, 19.20it/s]training_loss tensor(1.4145, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  33%|███▎      | 249/754 [00:12<00:26, 19.20it/s]training_loss tensor(1.0107, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  33%|███▎      | 250/754 [00:13<00:26, 19.20it/s]training_loss tensor(1.0903, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  33%|███▎      | 251/754 [00:13<00:26, 19.21it/s]training_loss tensor(0.9431, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  33%|███▎      | 252/754 [00:13<00:26, 19.21it/s]training_loss tensor(0.9708, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  34%|███▎      | 253/754 [00:13<00:26, 19.21it/s]training_loss tensor(1.0182, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  34%|███▎      | 254/754 [00:13<00:26, 19.22it/s]training_loss tensor(1.0452, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  34%|███▍      | 255/754 [00:13<00:25, 19.22it/s]training_loss tensor(0.9569, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  34%|███▍      | 256/754 [00:13<00:25, 19.22it/s]training_loss tensor(0.9196, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  34%|███▍      | 257/754 [00:13<00:25, 19.23it/s]training_loss tensor(1.0467, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  34%|███▍      | 258/754 [00:13<00:25, 19.23it/s]training_loss tensor(1.2522, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  34%|███▍      | 259/754 [00:13<00:25, 19.23it/s]training_loss tensor(1.3256, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  34%|███▍      | 260/754 [00:13<00:25, 19.23it/s]training_loss tensor(1.2556, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  35%|███▍      | 261/754 [00:13<00:25, 19.24it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.3224, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  35%|███▍      | 262/754 [00:13<00:25, 19.24it/s]training_loss tensor(1.2785, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  35%|███▍      | 263/754 [00:13<00:25, 19.24it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.2330, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  35%|███▌      | 264/754 [00:13<00:25, 19.25it/s]training_loss tensor(1.8576, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  35%|███▌      | 265/754 [00:13<00:25, 19.25it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(2.0023, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  35%|███▌      | 266/754 [00:13<00:25, 19.25it/s]training_loss tensor(2.0903, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  35%|███▌      | 267/754 [00:13<00:25, 19.26it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(2.1950, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  36%|███▌      | 268/754 [00:13<00:25, 19.26it/s]training_loss tensor(2.4049, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  36%|███▌      | 269/754 [00:13<00:25, 19.26it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.9774, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  36%|███▌      | 270/754 [00:14<00:25, 19.26it/s]training_loss tensor(1.9561, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  36%|███▌      | 271/754 [00:14<00:25, 19.27it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.9797, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  36%|███▌      | 272/754 [00:14<00:25, 19.27it/s]training_loss tensor(1.9684, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  36%|███▌      | 273/754 [00:14<00:24, 19.28it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.8524, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  36%|███▋      | 274/754 [00:14<00:24, 19.28it/s]training_loss tensor(1.9272, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  36%|███▋      | 275/754 [00:14<00:24, 19.28it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.8975, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  37%|███▋      | 276/754 [00:14<00:24, 19.28it/s]training_loss tensor(1.8301, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  37%|███▋      | 277/754 [00:14<00:24, 19.29it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.7561, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  37%|███▋      | 278/754 [00:14<00:24, 19.29it/s]training_loss tensor(1.8178, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  37%|███▋      | 279/754 [00:14<00:24, 19.29it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.9989, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  37%|███▋      | 280/754 [00:14<00:24, 19.29it/s]training_loss tensor(1.8701, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  37%|███▋      | 281/754 [00:14<00:24, 19.29it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.9315, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  37%|███▋      | 282/754 [00:14<00:24, 19.30it/s]training_loss tensor(2.0520, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  38%|███▊      | 283/754 [00:14<00:24, 19.30it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.9920, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  38%|███▊      | 284/754 [00:14<00:24, 19.30it/s]training_loss tensor(1.6221, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  38%|███▊      | 285/754 [00:14<00:24, 19.30it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.4643, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  38%|███▊      | 286/754 [00:14<00:24, 19.31it/s]training_loss tensor(1.2731, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  38%|███▊      | 287/754 [00:14<00:24, 19.31it/s]training_loss tensor(1.2948, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  38%|███▊      | 288/754 [00:14<00:24, 19.31it/s]training_loss tensor(1.3288, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  38%|███▊      | 289/754 [00:14<00:24, 19.32it/s]training_loss tensor(1.2745, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  38%|███▊      | 290/754 [00:15<00:24, 19.32it/s]training_loss tensor(1.2727, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  39%|███▊      | 291/754 [00:15<00:23, 19.32it/s]training_loss tensor(1.1811, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  39%|███▊      | 292/754 [00:15<00:23, 19.32it/s]training_loss tensor(1.2452, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  39%|███▉      | 293/754 [00:15<00:23, 19.33it/s]training_loss tensor(1.1361, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  39%|███▉      | 294/754 [00:15<00:23, 19.33it/s]training_loss tensor(1.0872, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  39%|███▉      | 295/754 [00:15<00:23, 19.33it/s]training_loss tensor(1.1681, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  39%|███▉      | 296/754 [00:15<00:23, 19.33it/s]training_loss tensor(1.2368, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  39%|███▉      | 297/754 [00:15<00:23, 19.33it/s]training_loss tensor(1.1381, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  40%|███▉      | 298/754 [00:15<00:23, 19.33it/s]training_loss tensor(1.2244, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  40%|███▉      | 299/754 [00:15<00:23, 19.33it/s]training_loss tensor(1.2526, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  40%|███▉      | 300/754 [00:15<00:23, 19.32it/s]training_loss tensor(1.1865, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  40%|███▉      | 301/754 [00:15<00:23, 19.32it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.0752, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  40%|████      | 302/754 [00:15<00:23, 19.32it/s]training_loss tensor(1.0261, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  40%|████      | 303/754 [00:15<00:23, 19.33it/s]training_loss \n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m tensor(1.0039, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  40%|████      | 304/754 [00:15<00:23, 19.33it/s]training_loss tensor(0.7865, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  40%|████      | 305/754 [00:15<00:23, 19.33it/s]training_loss tensor(0.7551, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  41%|████      | 306/754 [00:15<00:23, 19.34it/s]training_loss tensor(0.8344, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  41%|████      | 307/754 [00:15<00:23, 19.34it/s]training_loss tensor(0.8052, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  41%|████      | 308/754 [00:15<00:23, 19.33it/s]training_loss tensor(0.7209, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  41%|████      | 309/754 [00:15<00:23, 19.34it/s]training_loss tensor(0.6989, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  41%|████      | 310/754 [00:16<00:22, 19.34it/s]training_loss tensor(0.8406, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  41%|████      | 311/754 [00:16<00:22, 19.34it/s]training_loss tensor(0.8748, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  41%|████▏     | 312/754 [00:16<00:22, 19.34it/s]training_loss tensor(0.8645, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  42%|████▏     | 313/754 [00:16<00:22, 19.34it/s]training_loss tensor(1.0167, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  42%|████▏     | 314/754 [00:16<00:22, 19.34it/s]training_loss tensor(1.0150, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  42%|████▏     | 315/754 [00:16<00:22, 19.34it/s]training_loss tensor(1.0385, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  42%|████▏     | 316/754 [00:16<00:22, 19.34it/s]training_loss tensor(0.9173, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  42%|████▏     | 317/754 [00:16<00:22, 19.34it/s]training_loss tensor(0.8395, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  42%|████▏     | 318/754 [00:16<00:22, 19.35it/s]training_loss tensor(0.7689, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  42%|████▏     | 319/754 [00:16<00:22, 19.35it/s]training_loss tensor(0.8454, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  42%|████▏     | 320/754 [00:16<00:22, 19.35it/s]training_loss tensor(0.8221, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  43%|████▎     | 321/754 [00:16<00:22, 19.35it/s]training_loss tensor(0.8743, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  43%|████▎     | 322/754 [00:16<00:22, 19.35it/s]training_loss tensor(0.9891, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  43%|████▎     | 323/754 [00:16<00:22, 19.35it/s]training_loss tensor(0.9178, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  43%|████▎     | 324/754 [00:16<00:22, 19.35it/s]training_loss tensor(0.9415, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  43%|████▎     | 325/754 [00:16<00:22, 19.35it/s]training_loss tensor(0.9522, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  43%|████▎     | 326/754 [00:16<00:22, 19.36it/s]training_loss tensor(0.9260, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  43%|████▎     | 327/754 [00:16<00:22, 19.36it/s]training_loss tensor(0.8291, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  44%|████▎     | 328/754 [00:16<00:22, 19.36it/s]training_loss tensor(0.8232, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  44%|████▎     | 329/754 [00:16<00:21, 19.36it/s]training_loss tensor(0.8109, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  44%|████▍     | 330/754 [00:17<00:21, 19.37it/s]training_loss tensor(0.6812, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  44%|████▍     | 331/754 [00:17<00:21, 19.37it/s]training_loss tensor(0.5170, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  44%|████▍     | 332/754 [00:17<00:21, 19.37it/s]training_loss tensor(0.7323, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  44%|████▍     | 333/754 [00:17<00:21, 19.37it/s]training_loss tensor(0.7481, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  44%|████▍     | 334/754 [00:17<00:21, 19.37it/s]training_loss tensor(0.7057, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  44%|████▍     | 335/754 [00:17<00:21, 19.37it/s]training_loss tensor(0.6490, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  45%|████▍     | 336/754 [00:17<00:21, 19.38it/s]training_loss tensor(0.7149, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  45%|████▍     | 337/754 [00:17<00:21, 19.38it/s]training_loss tensor(0.6175, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  45%|████▍     | 338/754 [00:17<00:21, 19.38it/s]training_loss tensor(0.4705, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  45%|████▍     | 339/754 [00:17<00:21, 19.38it/s]training_loss tensor(0.4525, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  45%|████▌     | 340/754 [00:17<00:21, 19.38it/s]training_loss tensor(0.3854, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  45%|████▌     | 341/754 [00:17<00:21, 19.38it/s]training_loss tensor(0.4544, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  45%|████▌     | 342/754 [00:17<00:21, 19.38it/s]training_loss tensor(0.4821, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  45%|████▌     | 343/754 [00:17<00:21, 19.38it/s]training_loss tensor(0.4224, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  46%|████▌     | 344/754 [00:17<00:21, 19.39it/s]training_loss tensor(0.4682, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  46%|████▌     | 345/754 [00:17<00:21, 19.39it/s]training_loss tensor(0.4495, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  46%|████▌     | 346/754 [00:17<00:21, 19.39it/s]training_loss tensor(0.3692, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  46%|████▌     | 347/754 [00:17<00:20, 19.39it/s]training_loss tensor(0.2844, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  46%|████▌     | 348/754 [00:17<00:20, 19.39it/s]training_loss tensor(0.2762, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  46%|████▋     | 349/754 [00:17<00:20, 19.39it/s]training_loss tensor(0.4967, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  46%|████▋     | 350/754 [00:18<00:20, 19.40it/s]training_loss tensor(0.8196, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  47%|████▋     | 351/754 [00:18<00:20, 19.40it/s]training_loss tensor(1.1725, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  47%|████▋     | 352/754 [00:18<00:20, 19.40it/s]training_loss tensor(1.4463, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  47%|████▋     | 353/754 [00:18<00:20, 19.40it/s]training_loss tensor(1.6305, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  47%|████▋     | 354/754 [00:18<00:20, 19.41it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.6595, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  47%|████▋     | 355/754 [00:18<00:20, 19.40it/s]training_loss tensor(1.6159, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  47%|████▋     | 356/754 [00:18<00:20, 19.41it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.3724, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  47%|████▋     | 357/754 [00:18<00:20, 19.41it/s]training_loss tensor(1.2519, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  47%|████▋     | 358/754 [00:18<00:20, 19.40it/s]training_loss tensor(1.1940, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  48%|████▊     | 359/754 [00:18<00:20, 19.40it/s]training_loss tensor(1.1476, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  48%|████▊     | 360/754 [00:18<00:20, 19.40it/s]training_loss tensor(1.1748, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  48%|████▊     | 361/754 [00:18<00:20, 19.40it/s]training_loss tensor(1.2008, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  48%|████▊     | 362/754 [00:18<00:20, 19.40it/s]training_loss tensor(1.1659, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  48%|████▊     | 363/754 [00:18<00:20, 19.40it/s]training_loss tensor(1.0920, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  48%|████▊     | 364/754 [00:18<00:20, 19.40it/s]training_loss tensor(1.0135, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  48%|████▊     | 365/754 [00:18<00:20, 19.40it/s]training_loss tensor(0.9496, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  49%|████▊     | 366/754 [00:18<00:19, 19.40it/s]training_loss tensor(0.8366, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  49%|████▊     | 367/754 [00:18<00:19, 19.40it/s]training_loss tensor(0.7463, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  49%|████▉     | 368/754 [00:18<00:19, 19.40it/s]training_loss tensor(0.7176, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  49%|████▉     | 369/754 [00:19<00:19, 19.39it/s]training_loss tensor(0.4926, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  49%|████▉     | 370/754 [00:19<00:19, 19.39it/s]training_loss tensor(0.4156, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  49%|████▉     | 371/754 [00:19<00:19, 19.39it/s]training_loss tensor(0.5696, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  49%|████▉     | 372/754 [00:19<00:19, 19.40it/s]training_loss tensor(0.5346, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  49%|████▉     | 373/754 [00:19<00:19, 19.40it/s]training_loss tensor(0.6181, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  50%|████▉     | 374/754 [00:19<00:19, 19.39it/s]training_loss tensor(0.6712, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  50%|████▉     | 375/754 [00:19<00:19, 19.39it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.7887, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  50%|████▉     | 376/754 [00:19<00:19, 19.39it/s]training_loss tensor(0.7460, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  50%|█████     | 377/754 [00:19<00:19, 19.38it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.7090, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  50%|█████     | 378/754 [00:19<00:19, 19.38it/s]training_loss tensor(0.7066, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  50%|█████     | 379/754 [00:19<00:19, 19.38it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.6424, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  50%|█████     | 380/754 [00:19<00:19, 19.38it/s]training_loss tensor(0.4340, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  51%|█████     | 381/754 [00:19<00:19, 19.37it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.3867, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  51%|█████     | 382/754 [00:19<00:19, 19.37it/s]training_loss tensor(0.3348, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  51%|█████     | 383/754 [00:19<00:19, 19.36it/s]training_loss tensor(0.3280, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  51%|█████     | 384/754 [00:19<00:19, 19.36it/s]training_loss tensor(0.8723, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  51%|█████     | 385/754 [00:19<00:19, 19.36it/s]training_loss tensor(1.0025, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  51%|█████     | 386/754 [00:19<00:19, 19.37it/s]training_loss tensor(0.9923, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  51%|█████▏    | 387/754 [00:19<00:18, 19.36it/s]training_loss tensor(1.0134, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  51%|█████▏    | 388/754 [00:20<00:18, 19.37it/s]training_loss tensor(1.0449, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  52%|█████▏    | 389/754 [00:20<00:18, 19.36it/s]training_loss tensor(0.8307, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  52%|█████▏    | 390/754 [00:20<00:18, 19.36it/s]training_loss tensor(0.5876, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  52%|█████▏    | 391/754 [00:20<00:18, 19.35it/s]training_loss tensor(0.5469, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  52%|█████▏    | 392/754 [00:20<00:18, 19.35it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.5322, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  52%|█████▏    | 393/754 [00:20<00:18, 19.35it/s]training_loss tensor(0.5171, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  52%|█████▏    | 394/754 [00:20<00:18, 19.35it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.0483, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  52%|█████▏    | 395/754 [00:20<00:18, 19.35it/s]training_loss tensor(1.1552, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  53%|█████▎    | 396/754 [00:20<00:18, 19.35it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.3452, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  53%|█████▎    | 397/754 [00:20<00:18, 19.34it/s]training_loss tensor(1.3901, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  53%|█████▎    | 398/754 [00:20<00:18, 19.34it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.4013, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  53%|█████▎    | 399/754 [00:20<00:18, 19.34it/s]training_loss tensor(1.3017, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  53%|█████▎    | 400/754 [00:20<00:18, 19.34it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.1411, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  53%|█████▎    | 401/754 [00:20<00:18, 19.34it/s]training_loss tensor(0.9409, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  53%|█████▎    | 402/754 [00:20<00:18, 19.34it/s]training_loss tensor(0.9083, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  53%|█████▎    | 403/754 [00:20<00:18, 19.34it/s]training_loss tensor(0.8282, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  54%|█████▎    | 404/754 [00:20<00:18, 19.34it/s]training_loss tensor(0.8954, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  54%|█████▎    | 405/754 [00:20<00:18, 19.34it/s]training_loss tensor(1.0304, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  54%|█████▍    | 406/754 [00:20<00:17, 19.34it/s]training_loss tensor(1.5176, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  54%|█████▍    | 407/754 [00:21<00:17, 19.34it/s]training_loss tensor(1.6883, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  54%|█████▍    | 408/754 [00:21<00:17, 19.34it/s]training_loss tensor(1.6909, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  54%|█████▍    | 409/754 [00:21<00:17, 19.34it/s]training_loss tensor(1.7486, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  54%|█████▍    | 410/754 [00:21<00:17, 19.34it/s]training_loss tensor(1.8939, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  55%|█████▍    | 411/754 [00:21<00:17, 19.34it/s]training_loss tensor(1.6948, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  55%|█████▍    | 412/754 [00:21<00:17, 19.34it/s]training_loss tensor(1.5134, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  55%|█████▍    | 413/754 [00:21<00:17, 19.34it/s]training_loss tensor(1.4450, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  55%|█████▍    | 414/754 [00:21<00:17, 19.34it/s]training_loss tensor(1.3917, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  55%|█████▌    | 415/754 [00:21<00:17, 19.34it/s]training_loss tensor(1.1439, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  55%|█████▌    | 416/754 [00:21<00:17, 19.35it/s]training_loss tensor(0.7974, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  55%|█████▌    | 417/754 [00:21<00:17, 19.34it/s]training_loss tensor(0.8083, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  55%|█████▌    | 418/754 [00:21<00:17, 19.35it/s]training_loss tensor(0.9445, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  56%|█████▌    | 419/754 [00:21<00:17, 19.35it/s]training_loss tensor(0.9450, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  56%|█████▌    | 420/754 [00:21<00:17, 19.34it/s]training_loss tensor(0.9478, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  56%|█████▌    | 421/754 [00:21<00:17, 19.34it/s]training_loss tensor(0.8875, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  56%|█████▌    | 422/754 [00:21<00:17, 19.34it/s]training_loss tensor(0.9111, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  56%|█████▌    | 423/754 [00:21<00:17, 19.34it/s]training_loss tensor(0.7619, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  56%|█████▌    | 424/754 [00:21<00:17, 19.35it/s]training_loss tensor(0.6794, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  56%|█████▋    | 425/754 [00:21<00:17, 19.34it/s]training_loss tensor(0.6126, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  56%|█████▋    | 426/754 [00:22<00:16, 19.35it/s]training_loss tensor(0.5223, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  57%|█████▋    | 427/754 [00:22<00:16, 19.35it/s]training_loss tensor(0.4772, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  57%|█████▋    | 428/754 [00:22<00:16, 19.35it/s]training_loss tensor(0.8533, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  57%|█████▋    | 429/754 [00:22<00:16, 19.35it/s]training_loss tensor(0.8566, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  57%|█████▋    | 430/754 [00:22<00:16, 19.35it/s]training_loss tensor(0.8638, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  57%|█████▋    | 431/754 [00:22<00:16, 19.35it/s]training_loss tensor(0.8579, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  57%|█████▋    | 432/754 [00:22<00:16, 19.35it/s]training_loss tensor(0.9153, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  57%|█████▋    | 433/754 [00:22<00:16, 19.35it/s]training_loss tensor(0.6339, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  58%|█████▊    | 434/754 [00:22<00:16, 19.35it/s]training_loss tensor(0.4919, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  58%|█████▊    | 435/754 [00:22<00:16, 19.36it/s]training_loss tensor(0.7717, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  58%|█████▊    | 436/754 [00:22<00:16, 19.36it/s]training_loss tensor(1.1320, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  58%|█████▊    | 437/754 [00:22<00:16, 19.36it/s]training_loss tensor(1.0994, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  58%|█████▊    | 438/754 [00:22<00:16, 19.36it/s]training_loss tensor(1.1179, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  58%|█████▊    | 439/754 [00:22<00:16, 19.36it/s]training_loss tensor(1.1129, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  58%|█████▊    | 440/754 [00:22<00:16, 19.37it/s]training_loss tensor(1.0784, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  58%|█████▊    | 441/754 [00:22<00:16, 19.37it/s]training_loss tensor(0.5921, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  59%|█████▊    | 442/754 [00:22<00:16, 19.37it/s]\n",
      "Epoch 0:  59%|█████▊    | 442/754 [00:22<00:16, 19.37it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.6173, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  59%|█████▉    | 443/754 [00:22<00:16, 19.37it/s]training_loss tensor(0.7452, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  59%|█████▉    | 444/754 [00:22<00:16, 19.37it/s]training_loss tensor(0.7550, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  59%|█████▉    | 445/754 [00:22<00:15, 19.37it/s]training_loss tensor(0.7489, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  59%|█████▉    | 446/754 [00:23<00:15, 19.37it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.7217, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  59%|█████▉    | 447/754 [00:23<00:15, 19.38it/s]training_loss tensor(0.6843, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  59%|█████▉    | 448/754 [00:23<00:15, 19.38it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.5349, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  60%|█████▉    | 449/754 [00:23<00:15, 19.38it/s]training_loss tensor(0.6774, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  60%|█████▉    | 450/754 [00:23<00:15, 19.38it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.7528, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  60%|█████▉    | 451/754 [00:23<00:15, 19.38it/s]training_loss tensor(0.6999, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  60%|█████▉    | 452/754 [00:23<00:15, 19.38it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.7037, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  60%|██████    | 453/754 [00:23<00:15, 19.38it/s]training_loss tensor(0.7790, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  60%|██████    | 454/754 [00:23<00:15, 19.38it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.7339, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  60%|██████    | 455/754 [00:23<00:15, 19.38it/s]training_loss tensor(0.6825, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  60%|██████    | 456/754 [00:23<00:15, 19.39it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.7695, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  61%|██████    | 457/754 [00:23<00:15, 19.38it/s]training_loss tensor(0.7986, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  61%|██████    | 458/754 [00:23<00:15, 19.39it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.7783, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  61%|██████    | 459/754 [00:23<00:15, 19.39it/s]training_loss tensor(0.7777, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  61%|██████    | 460/754 [00:23<00:15, 19.39it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.7043, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  61%|██████    | 461/754 [00:23<00:15, 19.39it/s]training_loss tensor(0.6005, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  61%|██████▏   | 462/754 [00:23<00:15, 19.39it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.5823, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  61%|██████▏   | 463/754 [00:23<00:15, 19.39it/s]training_loss tensor(0.5770, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  62%|██████▏   | 464/754 [00:23<00:14, 19.39it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.6447, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  62%|██████▏   | 465/754 [00:23<00:14, 19.39it/s]training_loss tensor(0.6745, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  62%|██████▏   | 466/754 [00:24<00:14, 19.39it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.7232, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  62%|██████▏   | 467/754 [00:24<00:14, 19.39it/s]training_loss tensor(0.7735, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  62%|██████▏   | 468/754 [00:24<00:14, 19.40it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.7330, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  62%|██████▏   | 469/754 [00:24<00:14, 19.40it/s]training_loss tensor(0.7991, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  62%|██████▏   | 470/754 [00:24<00:14, 19.40it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(2.1082, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  62%|██████▏   | 471/754 [00:24<00:14, 19.40it/s]training_loss tensor(2.8747, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  63%|██████▎   | 472/754 [00:24<00:14, 19.40it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(3.0024, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  63%|██████▎   | 473/754 [00:24<00:14, 19.40it/s]training_loss tensor(3.0861, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  63%|██████▎   | 474/754 [00:24<00:14, 19.40it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(3.0885, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  63%|██████▎   | 475/754 [00:24<00:14, 19.40it/s]training_loss tensor(2.8391, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  63%|██████▎   | 476/754 [00:24<00:14, 19.40it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.4474, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  63%|██████▎   | 477/754 [00:24<00:14, 19.41it/s]training_loss tensor(1.2562, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  63%|██████▎   | 478/754 [00:24<00:14, 19.41it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.9979, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  64%|██████▎   | 479/754 [00:24<00:14, 19.41it/s]training_loss tensor(0.9950, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  64%|██████▎   | 480/754 [00:24<00:14, 19.41it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.8648, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  64%|██████▍   | 481/754 [00:24<00:14, 19.41it/s]training_loss tensor(0.8591, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  64%|██████▍   | 482/754 [00:24<00:14, 19.41it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.9124, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  64%|██████▍   | 483/754 [00:24<00:13, 19.41it/s]training_loss tensor(0.9499, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  64%|██████▍   | 484/754 [00:24<00:13, 19.41it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.9640, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  64%|██████▍   | 485/754 [00:24<00:13, 19.41it/s]training_loss tensor(1.0364, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  64%|██████▍   | 486/754 [00:25<00:13, 19.42it/s]training_loss tensor(1.0832, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  65%|██████▍   | 487/754 [00:25<00:13, 19.42it/s]training_loss tensor(1.0635, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  65%|██████▍   | 488/754 [00:25<00:13, 19.42it/s]training_loss tensor(0.9454, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  65%|██████▍   | 489/754 [00:25<00:13, 19.42it/s]training_loss tensor(1.0117, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  65%|██████▍   | 490/754 [00:25<00:13, 19.42it/s]training_loss tensor(0.8662, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  65%|██████▌   | 491/754 [00:25<00:13, 19.42it/s]training_loss tensor(0.8846, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  65%|██████▌   | 492/754 [00:25<00:13, 19.42it/s]training_loss tensor(0.8232, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  65%|██████▌   | 493/754 [00:25<00:13, 19.42it/s]training_loss tensor(0.8096, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  66%|██████▌   | 494/754 [00:25<00:13, 19.42it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.7870, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  66%|██████▌   | 495/754 [00:25<00:13, 19.42it/s]training_loss tensor(0.7295, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  66%|██████▌   | 496/754 [00:25<00:13, 19.42it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.5186, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  66%|██████▌   | 497/754 [00:25<00:13, 19.42it/s]training_loss tensor(0.5357, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  66%|██████▌   | 498/754 [00:25<00:13, 19.42it/s]training_loss tensor(0.4689, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  66%|██████▌   | 499/754 [00:25<00:13, 19.42it/s]training_loss tensor(0.4039, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  66%|██████▋   | 500/754 [00:25<00:13, 19.42it/s]training_loss tensor(0.4250, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  66%|██████▋   | 501/754 [00:25<00:13, 19.43it/s]training_loss tensor(0.3919, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  67%|██████▋   | 502/754 [00:25<00:12, 19.43it/s]training_loss tensor(0.3359, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  67%|██████▋   | 503/754 [00:25<00:12, 19.43it/s]training_loss tensor(0.3249, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  67%|██████▋   | 504/754 [00:25<00:12, 19.43it/s]training_loss tensor(0.4437, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  67%|██████▋   | 505/754 [00:25<00:12, 19.43it/s]training_loss tensor(0.6550, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  67%|██████▋   | 506/754 [00:26<00:12, 19.43it/s]training_loss tensor(0.7536, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  67%|██████▋   | 507/754 [00:26<00:12, 19.43it/s]training_loss tensor(0.8515, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  67%|██████▋   | 508/754 [00:26<00:12, 19.43it/s]training_loss tensor(0.8481, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  68%|██████▊   | 509/754 [00:26<00:12, 19.43it/s]training_loss tensor(0.8678, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  68%|██████▊   | 510/754 [00:26<00:12, 19.43it/s]training_loss tensor(0.6612, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  68%|██████▊   | 511/754 [00:26<00:12, 19.43it/s]training_loss tensor(0.5976, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  68%|██████▊   | 512/754 [00:26<00:12, 19.43it/s]training_loss tensor(0.5620, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  68%|██████▊   | 513/754 [00:26<00:12, 19.43it/s]training_loss tensor(0.5138, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  68%|██████▊   | 514/754 [00:26<00:12, 19.43it/s]training_loss tensor(0.7653, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  68%|██████▊   | 515/754 [00:26<00:12, 19.43it/s]training_loss tensor(0.8550, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  68%|██████▊   | 516/754 [00:26<00:12, 19.43it/s]training_loss tensor(0.8286, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  69%|██████▊   | 517/754 [00:26<00:12, 19.43it/s]training_loss tensor(0.8505, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  69%|██████▊   | 518/754 [00:26<00:12, 19.43it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.8255, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  69%|██████▉   | 519/754 [00:26<00:12, 19.42it/s]training_loss tensor(0.7214, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  69%|██████▉   | 520/754 [00:26<00:12, 19.42it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.5179, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  69%|██████▉   | 521/754 [00:26<00:11, 19.42it/s]training_loss tensor(0.5328, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  69%|██████▉   | 522/754 [00:26<00:11, 19.42it/s]training_loss tensor(0.5043, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  69%|██████▉   | 523/754 [00:26<00:11, 19.42it/s]training_loss tensor(0.5129, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  69%|██████▉   | 524/754 [00:26<00:11, 19.42it/s]training_loss tensor(0.4326, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  70%|██████▉   | 525/754 [00:27<00:11, 19.42it/s]training_loss tensor(0.3650, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  70%|██████▉   | 526/754 [00:27<00:11, 19.42it/s]training_loss tensor(0.5197, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  70%|██████▉   | 527/754 [00:27<00:11, 19.42it/s]training_loss tensor(0.5397, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  70%|███████   | 528/754 [00:27<00:11, 19.41it/s]training_loss tensor(0.6032, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  70%|███████   | 529/754 [00:27<00:11, 19.41it/s]training_loss tensor(0.6229, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  70%|███████   | 530/754 [00:27<00:11, 19.41it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.8522, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  70%|███████   | 531/754 [00:27<00:11, 19.40it/s]training_loss tensor(0.8488, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  71%|███████   | 532/754 [00:27<00:11, 19.40it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.8214, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  71%|███████   | 533/754 [00:27<00:11, 19.40it/s]training_loss tensor(0.8880, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  71%|███████   | 534/754 [00:27<00:11, 19.40it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.9002, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  71%|███████   | 535/754 [00:27<00:11, 19.40it/s]training_loss tensor(0.8911, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  71%|███████   | 536/754 [00:27<00:11, 19.40it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.9651, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  71%|███████   | 537/754 [00:27<00:11, 19.39it/s]training_loss tensor(0.9542, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  71%|███████▏  | 538/754 [00:27<00:11, 19.39it/s]training_loss tensor(0.9264, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  71%|███████▏  | 539/754 [00:27<00:11, 19.39it/s]training_loss tensor(0.9270, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  72%|███████▏  | 540/754 [00:27<00:11, 19.39it/s]training_loss tensor(0.9934, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  72%|███████▏  | 541/754 [00:27<00:10, 19.39it/s]training_loss tensor(0.9272, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  72%|███████▏  | 542/754 [00:27<00:10, 19.39it/s]training_loss tensor(0.9532, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  72%|███████▏  | 543/754 [00:28<00:10, 19.39it/s]training_loss tensor(0.9514, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  72%|███████▏  | 544/754 [00:28<00:10, 19.38it/s]training_loss tensor(1.0466, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  72%|███████▏  | 545/754 [00:28<00:10, 19.38it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.1250, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  72%|███████▏  | 546/754 [00:28<00:10, 19.38it/s]training_loss tensor(1.1442, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  73%|███████▎  | 547/754 [00:28<00:10, 19.38it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.3098, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  73%|███████▎  | 548/754 [00:28<00:10, 19.38it/s]training_loss tensor(1.3236, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  73%|███████▎  | 549/754 [00:28<00:10, 19.38it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.4851, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  73%|███████▎  | 550/754 [00:28<00:10, 19.37it/s]training_loss tensor(1.5161, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  73%|███████▎  | 551/754 [00:28<00:10, 19.37it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.4682, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  73%|███████▎  | 552/754 [00:28<00:10, 19.37it/s]training_loss tensor(1.3870, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  73%|███████▎  | 553/754 [00:28<00:10, 19.38it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.3823, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  73%|███████▎  | 554/754 [00:28<00:10, 19.38it/s]training_loss tensor(1.5081, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  74%|███████▎  | 555/754 [00:28<00:10, 19.38it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.3917, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  74%|███████▎  | 556/754 [00:28<00:10, 19.38it/s]training_loss tensor(1.3722, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  74%|███████▍  | 557/754 [00:28<00:10, 19.38it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.3775, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  74%|███████▍  | 558/754 [00:28<00:10, 19.38it/s]training_loss tensor(1.5017, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  74%|███████▍  | 559/754 [00:28<00:10, 19.38it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.2372, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  74%|███████▍  | 560/754 [00:28<00:10, 19.38it/s]training_loss tensor(1.2439, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  74%|███████▍  | 561/754 [00:28<00:09, 19.38it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.2349, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  75%|███████▍  | 562/754 [00:29<00:09, 19.38it/s]training_loss tensor(1.1253, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  75%|███████▍  | 563/754 [00:29<00:09, 19.38it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.9893, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  75%|███████▍  | 564/754 [00:29<00:09, 19.38it/s]training_loss tensor(0.9215, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  75%|███████▍  | 565/754 [00:29<00:09, 19.38it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.9156, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  75%|███████▌  | 566/754 [00:29<00:09, 19.38it/s]training_loss tensor(0.8655, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  75%|███████▌  | 567/754 [00:29<00:09, 19.38it/s]\n",
      "Epoch 0:  75%|███████▌  | 567/754 [00:29<00:09, 19.38it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.7430, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  75%|███████▌  | 568/754 [00:29<00:09, 19.38it/s]training_loss tensor(0.7026, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  75%|███████▌  | 569/754 [00:29<00:09, 19.38it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.6994, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  76%|███████▌  | 570/754 [00:29<00:09, 19.38it/s]training_loss tensor(0.6986, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  76%|███████▌  | 571/754 [00:29<00:09, 19.38it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.7700, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  76%|███████▌  | 572/754 [00:29<00:09, 19.38it/s]training_loss tensor(0.8149, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  76%|███████▌  | 573/754 [00:29<00:09, 19.38it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.0179, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  76%|███████▌  | 574/754 [00:29<00:09, 19.38it/s]training_loss tensor(1.0280, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  76%|███████▋  | 575/754 [00:29<00:09, 19.38it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.1263, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  76%|███████▋  | 576/754 [00:29<00:09, 19.38it/s]training_loss tensor(1.0911, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  77%|███████▋  | 577/754 [00:29<00:09, 19.38it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.2905, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  77%|███████▋  | 578/754 [00:29<00:09, 19.39it/s]training_loss tensor(1.3833, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  77%|███████▋  | 579/754 [00:29<00:09, 19.39it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.6625, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  77%|███████▋  | 580/754 [00:29<00:08, 19.39it/s]training_loss tensor(1.7646, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  77%|███████▋  | 581/754 [00:29<00:08, 19.39it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.7964, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  77%|███████▋  | 582/754 [00:30<00:08, 19.39it/s]training_loss tensor(1.6700, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  77%|███████▋  | 583/754 [00:30<00:08, 19.39it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.6481, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  77%|███████▋  | 584/754 [00:30<00:08, 19.39it/s]training_loss tensor(1.3874, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  78%|███████▊  | 585/754 [00:30<00:08, 19.39it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.3049, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  78%|███████▊  | 586/754 [00:30<00:08, 19.39it/s]training_loss tensor(1.2718, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  78%|███████▊  | 587/754 [00:30<00:08, 19.39it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.3886, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  78%|███████▊  | 588/754 [00:30<00:08, 19.38it/s]training_loss tensor(1.4550, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  78%|███████▊  | 589/754 [00:30<00:08, 19.39it/s]training_loss tensor(1.3378, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  78%|███████▊  | 590/754 [00:30<00:08, 19.38it/s]training_loss tensor(1.2167, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  78%|███████▊  | 591/754 [00:30<00:08, 19.38it/s]training_loss tensor(1.1918, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  79%|███████▊  | 592/754 [00:30<00:08, 19.38it/s]training_loss tensor(1.0189, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  79%|███████▊  | 593/754 [00:30<00:08, 19.38it/s]training_loss tensor(0.8182, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  79%|███████▉  | 594/754 [00:30<00:08, 19.39it/s]training_loss tensor(0.6953, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  79%|███████▉  | 595/754 [00:30<00:08, 19.39it/s]training_loss tensor(0.7467, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  79%|███████▉  | 596/754 [00:30<00:08, 19.39it/s]training_loss tensor(0.8144, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  79%|███████▉  | 597/754 [00:30<00:08, 19.39it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.8652, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  79%|███████▉  | 598/754 [00:30<00:08, 19.39it/s]training_loss tensor(0.9279, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  79%|███████▉  | 599/754 [00:30<00:07, 19.39it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.9647, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  80%|███████▉  | 600/754 [00:30<00:07, 19.39it/s]training_loss tensor(0.9442, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  80%|███████▉  | 601/754 [00:30<00:07, 19.40it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.9039, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  80%|███████▉  | 602/754 [00:31<00:07, 19.40it/s]training_loss tensor(0.8889, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  80%|███████▉  | 603/754 [00:31<00:07, 19.40it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.8897, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  80%|████████  | 604/754 [00:31<00:07, 19.40it/s]training_loss tensor(0.8328, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  80%|████████  | 605/754 [00:31<00:07, 19.40it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.7854, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  80%|████████  | 606/754 [00:31<00:07, 19.40it/s]training_loss tensor(0.8785, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  81%|████████  | 607/754 [00:31<00:07, 19.40it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.9443, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  81%|████████  | 608/754 [00:31<00:07, 19.40it/s]training_loss tensor(0.9745, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  81%|████████  | 609/754 [00:31<00:07, 19.41it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.9636, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  81%|████████  | 610/754 [00:31<00:07, 19.41it/s]training_loss tensor(1.1506, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  81%|████████  | 611/754 [00:31<00:07, 19.41it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.1257, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  81%|████████  | 612/754 [00:31<00:07, 19.41it/s]training_loss tensor(1.0463, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  81%|████████▏ | 613/754 [00:31<00:07, 19.41it/s]training_loss tensor(1.1147, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  81%|████████▏ | 614/754 [00:31<00:07, 19.41it/s]training_loss tensor(1.1961, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  82%|████████▏ | 615/754 [00:31<00:07, 19.41it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.0332, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  82%|████████▏ | 616/754 [00:31<00:07, 19.41it/s]training_loss tensor(1.0036, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  82%|████████▏ | 617/754 [00:31<00:07, 19.41it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.9899, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  82%|████████▏ | 618/754 [00:31<00:07, 19.41it/s]training_loss tensor(0.9281, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  82%|████████▏ | 619/754 [00:31<00:06, 19.41it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.8619, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  82%|████████▏ | 620/754 [00:31<00:06, 19.41it/s]training_loss tensor(0.8146, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  82%|████████▏ | 621/754 [00:31<00:06, 19.41it/s]training_loss \n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m tensor(0.7346, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  82%|████████▏ | 622/754 [00:32<00:06, 19.42it/s]training_loss tensor(0.7534, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  83%|████████▎ | 623/754 [00:32<00:06, 19.42it/s]training_loss tensor(0.7435, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  83%|████████▎ | 624/754 [00:32<00:06, 19.42it/s]training_loss tensor(0.7160, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  83%|████████▎ | 625/754 [00:32<00:06, 19.42it/s]training_loss tensor(0.7754, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  83%|████████▎ | 626/754 [00:32<00:06, 19.42it/s]training_loss tensor(0.8567, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  83%|████████▎ | 627/754 [00:32<00:06, 19.42it/s]training_loss tensor(0.8613, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  83%|████████▎ | 628/754 [00:32<00:06, 19.42it/s]training_loss tensor(0.9367, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  83%|████████▎ | 629/754 [00:32<00:06, 19.42it/s]training_loss tensor(0.9610, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  84%|████████▎ | 630/754 [00:32<00:06, 19.42it/s]training_loss tensor(0.9571, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  84%|████████▎ | 631/754 [00:32<00:06, 19.42it/s]training_loss tensor(0.9998, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  84%|████████▍ | 632/754 [00:32<00:06, 19.42it/s]training_loss tensor(0.9791, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  84%|████████▍ | 633/754 [00:32<00:06, 19.42it/s]training_loss tensor(0.9396, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  84%|████████▍ | 634/754 [00:32<00:06, 19.42it/s]training_loss tensor(0.8283, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  84%|████████▍ | 635/754 [00:32<00:06, 19.43it/s]training_loss tensor(0.8160, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  84%|████████▍ | 636/754 [00:32<00:06, 19.43it/s]training_loss tensor(0.7029, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  84%|████████▍ | 637/754 [00:32<00:06, 19.43it/s]training_loss tensor(0.6964, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  85%|████████▍ | 638/754 [00:32<00:05, 19.43it/s]training_loss tensor(0.6808, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  85%|████████▍ | 639/754 [00:32<00:05, 19.43it/s]training_loss tensor(0.6710, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  85%|████████▍ | 640/754 [00:32<00:05, 19.43it/s]training_loss tensor(0.7200, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  85%|████████▌ | 641/754 [00:32<00:05, 19.43it/s]training_loss tensor(0.8647, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  85%|████████▌ | 642/754 [00:33<00:05, 19.43it/s]training_loss tensor(0.7839, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  85%|████████▌ | 643/754 [00:33<00:05, 19.44it/s]training_loss tensor(0.8281, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  85%|████████▌ | 644/754 [00:33<00:05, 19.44it/s]training_loss tensor(1.0315, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  86%|████████▌ | 645/754 [00:33<00:05, 19.44it/s]training_loss tensor(0.9743, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  86%|████████▌ | 646/754 [00:33<00:05, 19.44it/s]training_loss tensor(0.8047, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  86%|████████▌ | 647/754 [00:33<00:05, 19.44it/s]training_loss tensor(0.9240, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  86%|████████▌ | 648/754 [00:33<00:05, 19.44it/s]training_loss tensor(0.9903, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  86%|████████▌ | 649/754 [00:33<00:05, 19.44it/s]training_loss tensor(0.8428, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  86%|████████▌ | 650/754 [00:33<00:05, 19.44it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.1283, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  86%|████████▋ | 651/754 [00:33<00:05, 19.44it/s]training_loss tensor(1.1970, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  86%|████████▋ | 652/754 [00:33<00:05, 19.45it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.2351, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  87%|████████▋ | 653/754 [00:33<00:05, 19.45it/s]training_loss tensor(1.2167, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  87%|████████▋ | 654/754 [00:33<00:05, 19.45it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.2268, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  87%|████████▋ | 655/754 [00:33<00:05, 19.45it/s]training_loss tensor(1.0156, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  87%|████████▋ | 656/754 [00:33<00:05, 19.45it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.8558, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  87%|████████▋ | 657/754 [00:33<00:04, 19.45it/s]training_loss tensor(0.7587, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  87%|████████▋ | 658/754 [00:33<00:04, 19.45it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.6170, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  87%|████████▋ | 659/754 [00:33<00:04, 19.45it/s]training_loss tensor(0.6086, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  88%|████████▊ | 660/754 [00:33<00:04, 19.45it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.6664, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  88%|████████▊ | 661/754 [00:33<00:04, 19.46it/s]training_loss tensor(0.7420, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  88%|████████▊ | 662/754 [00:34<00:04, 19.46it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.6887, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  88%|████████▊ | 663/754 [00:34<00:04, 19.46it/s]training_loss tensor(0.6911, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  88%|████████▊ | 664/754 [00:34<00:04, 19.46it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.7240, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  88%|████████▊ | 665/754 [00:34<00:04, 19.46it/s]training_loss tensor(0.7168, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  88%|████████▊ | 666/754 [00:34<00:04, 19.46it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.7325, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  88%|████████▊ | 667/754 [00:34<00:04, 19.46it/s]training_loss tensor(0.9638, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  89%|████████▊ | 668/754 [00:34<00:04, 19.46it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.1539, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  89%|████████▊ | 669/754 [00:34<00:04, 19.46it/s]training_loss tensor(1.1651, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  89%|████████▉ | 670/754 [00:34<00:04, 19.46it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.2170, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  89%|████████▉ | 671/754 [00:34<00:04, 19.46it/s]training_loss tensor(1.1909, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  89%|████████▉ | 672/754 [00:34<00:04, 19.46it/s]training_loss tensor(1.0849, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  89%|████████▉ | 673/754 [00:34<00:04, 19.46it/s]training_loss tensor(0.9384, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  89%|████████▉ | 674/754 [00:34<00:04, 19.46it/s]training_loss tensor(0.9190, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  90%|████████▉ | 675/754 [00:34<00:04, 19.47it/s]training_loss tensor(0.8483, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  90%|████████▉ | 676/754 [00:34<00:04, 19.47it/s]training_loss tensor(1.2279, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  90%|████████▉ | 677/754 [00:34<00:03, 19.47it/s]training_loss tensor(1.4901, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  90%|████████▉ | 678/754 [00:34<00:03, 19.47it/s]training_loss tensor(1.4912, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  90%|█████████ | 679/754 [00:34<00:03, 19.47it/s]training_loss tensor(1.4443, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  90%|█████████ | 680/754 [00:34<00:03, 19.47it/s]training_loss tensor(1.4733, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  90%|█████████ | 681/754 [00:34<00:03, 19.47it/s]training_loss tensor(1.2474, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  90%|█████████ | 682/754 [00:35<00:03, 19.47it/s]training_loss tensor(0.8910, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  91%|█████████ | 683/754 [00:35<00:03, 19.47it/s]training_loss tensor(0.7158, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  91%|█████████ | 684/754 [00:35<00:03, 19.47it/s]training_loss tensor(0.6807, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  91%|█████████ | 685/754 [00:35<00:03, 19.47it/s]training_loss tensor(0.8534, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  91%|█████████ | 686/754 [00:35<00:03, 19.47it/s]training_loss tensor(0.8702, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  91%|█████████ | 687/754 [00:35<00:03, 19.47it/s]training_loss tensor(0.9036, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  91%|█████████ | 688/754 [00:35<00:03, 19.47it/s]training_loss tensor(0.9080, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  91%|█████████▏| 689/754 [00:35<00:03, 19.47it/s]training_loss tensor(0.8230, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  92%|█████████▏| 690/754 [00:35<00:03, 19.47it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.7345, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  92%|█████████▏| 691/754 [00:35<00:03, 19.46it/s]training_loss tensor(0.7273, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  92%|█████████▏| 692/754 [00:35<00:03, 19.46it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.5359, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  92%|█████████▏| 693/754 [00:35<00:03, 19.46it/s]training_loss tensor(0.7640, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  92%|█████████▏| 694/754 [00:35<00:03, 19.46it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.7783, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  92%|█████████▏| 695/754 [00:35<00:03, 19.46it/s]training_loss tensor(0.8295, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  92%|█████████▏| 696/754 [00:35<00:02, 19.46it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.8203, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  92%|█████████▏| 697/754 [00:35<00:02, 19.46it/s]training_loss tensor(0.8048, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  93%|█████████▎| 698/754 [00:35<00:02, 19.46it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.8612, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  93%|█████████▎| 699/754 [00:35<00:02, 19.46it/s]training_loss tensor(0.9806, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  93%|█████████▎| 700/754 [00:35<00:02, 19.45it/s]training_loss tensor(0.9495, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  93%|█████████▎| 701/754 [00:36<00:02, 19.45it/s]training_loss tensor(0.9892, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  93%|█████████▎| 702/754 [00:36<00:02, 19.45it/s]training_loss tensor(1.0184, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  93%|█████████▎| 703/754 [00:36<00:02, 19.44it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.8729, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  93%|█████████▎| 704/754 [00:36<00:02, 19.44it/s]training_loss tensor(0.6936, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  94%|█████████▎| 705/754 [00:36<00:02, 19.44it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.5554, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  94%|█████████▎| 706/754 [00:36<00:02, 19.44it/s]training_loss tensor(0.4843, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  94%|█████████▍| 707/754 [00:36<00:02, 19.44it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.4062, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  94%|█████████▍| 708/754 [00:36<00:02, 19.44it/s]training_loss tensor(0.4029, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  94%|█████████▍| 709/754 [00:36<00:02, 19.44it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.3903, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  94%|█████████▍| 710/754 [00:36<00:02, 19.44it/s]training_loss tensor(0.4594, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  94%|█████████▍| 711/754 [00:36<00:02, 19.44it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.4333, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  94%|█████████▍| 712/754 [00:36<00:02, 19.43it/s]training_loss tensor(0.4918, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  95%|█████████▍| 713/754 [00:36<00:02, 19.43it/s]training_loss tensor(0.9990, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  95%|█████████▍| 714/754 [00:36<00:02, 19.43it/s]training_loss tensor(1.3152, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  95%|█████████▍| 715/754 [00:36<00:02, 19.43it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.3068, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  95%|█████████▍| 716/754 [00:36<00:01, 19.43it/s]training_loss tensor(1.3025, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  95%|█████████▌| 717/754 [00:36<00:01, 19.43it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.3275, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  95%|█████████▌| 718/754 [00:36<00:01, 19.43it/s]training_loss tensor(1.1824, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  95%|█████████▌| 719/754 [00:37<00:01, 19.43it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.4860, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  95%|█████████▌| 720/754 [00:37<00:01, 19.43it/s]training_loss tensor(0.4657, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  96%|█████████▌| 721/754 [00:37<00:01, 19.43it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.4717, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  96%|█████████▌| 722/754 [00:37<00:01, 19.43it/s]training_loss tensor(0.4107, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  96%|█████████▌| 723/754 [00:37<00:01, 19.43it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.5314, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  96%|█████████▌| 724/754 [00:37<00:01, 19.43it/s]training_loss tensor(0.4633, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  96%|█████████▌| 725/754 [00:37<00:01, 19.43it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.4375, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  96%|█████████▋| 726/754 [00:37<00:01, 19.43it/s]training_loss tensor(0.4311, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  96%|█████████▋| 727/754 [00:37<00:01, 19.43it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.4063, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  97%|█████████▋| 728/754 [00:37<00:01, 19.43it/s]training_loss tensor(0.2495, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  97%|█████████▋| 729/754 [00:37<00:01, 19.43it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.3417, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  97%|█████████▋| 730/754 [00:37<00:01, 19.43it/s]training_loss tensor(0.5759, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  97%|█████████▋| 731/754 [00:37<00:01, 19.43it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.6907, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  97%|█████████▋| 732/754 [00:37<00:01, 19.43it/s]training_loss tensor(0.7275, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  97%|█████████▋| 733/754 [00:37<00:01, 19.43it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.7308, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  97%|█████████▋| 734/754 [00:37<00:01, 19.43it/s]training_loss tensor(0.7846, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  97%|█████████▋| 735/754 [00:37<00:00, 19.43it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.6114, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  98%|█████████▊| 736/754 [00:37<00:00, 19.43it/s]training_loss tensor(0.5888, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  98%|█████████▊| 737/754 [00:37<00:00, 19.43it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.5360, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  98%|█████████▊| 738/754 [00:37<00:00, 19.43it/s]training_loss tensor(0.7560, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  98%|█████████▊| 739/754 [00:38<00:00, 19.43it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.7097, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  98%|█████████▊| 740/754 [00:38<00:00, 19.43it/s]training_loss tensor(0.7666, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  98%|█████████▊| 741/754 [00:38<00:00, 19.43it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.7436, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  98%|█████████▊| 742/754 [00:38<00:00, 19.43it/s]training_loss tensor(0.7553, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  99%|█████████▊| 743/754 [00:38<00:00, 19.43it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.6408, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  99%|█████████▊| 744/754 [00:38<00:00, 19.43it/s]training_loss tensor(0.8098, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  99%|█████████▉| 745/754 [00:38<00:00, 19.43it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.9687, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  99%|█████████▉| 746/754 [00:38<00:00, 19.44it/s]training_loss tensor(1.0527, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  99%|█████████▉| 747/754 [00:38<00:00, 19.44it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.0391, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  99%|█████████▉| 748/754 [00:38<00:00, 19.44it/s]training_loss tensor(1.0537, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  99%|█████████▉| 749/754 [00:38<00:00, 19.44it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.9424, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0:  99%|█████████▉| 750/754 [00:38<00:00, 19.44it/s]training_loss tensor(0.7500, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0: 100%|█████████▉| 751/754 [00:38<00:00, 19.44it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.5405, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0: 100%|█████████▉| 752/754 [00:38<00:00, 19.44it/s]training_loss tensor(0.4907, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0: 100%|█████████▉| 753/754 [00:38<00:00, 19.44it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.4926, grad_fn=<SqrtBackward0>)\n",
      "Epoch 0: 100%|██████████| 754/754 [00:38<00:00, 19.44it/s]\n",
      "Validation: 0it [00:00, ?it/s]\u001B[A1571)\u001B[0m \n",
      "Validation:   0%|          | 0/5 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:  20%|██        | 1/5 [00:00<00:00, 87.52it/s]\u001B[A\n",
      "Validation DataLoader 0:  40%|████      | 2/5 [00:00<00:00, 92.47it/s]\u001B[A\n",
      "Validation DataLoader 0:  60%|██████    | 3/5 [00:00<00:00, 93.92it/s]\u001B[A\n",
      "Validation DataLoader 0:  80%|████████  | 4/5 [00:00<00:00, 96.50it/s]\u001B[A\n",
      "Validation DataLoader 0: 100%|██████████| 5/5 [00:00<00:00, 96.11it/s]\u001B[A\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<div class=\"trialProgress\">\n  <h3>Trial Progress</h3>\n  <table>\n<thead>\n<tr><th>Trial name                   </th><th>date               </th><th>done  </th><th>hostname     </th><th style=\"text-align: right;\">  iterations_since_restore</th><th style=\"text-align: right;\">   loss</th><th>node_ip        </th><th style=\"text-align: right;\">  pid</th><th>should_checkpoint  </th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  training_iteration</th><th style=\"text-align: right;\">   trial_id</th></tr>\n</thead>\n<tbody>\n<tr><td>train_transformer_43222_00000</td><td>2023-06-02_19-48-58</td><td>False </td><td>o-dubchak-pc2</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.60245</td><td>192.168.248.158</td><td style=\"text-align: right;\">11571</td><td>True               </td><td style=\"text-align: right;\">              40.218</td><td style=\"text-align: right;\">            40.218</td><td style=\"text-align: right;\">        40.218</td><td style=\"text-align: right;\"> 1685724538</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">43222_00000</td></tr>\n</tbody>\n</table>\n</div>\n<style>\n.trialProgress {\n  display: flex;\n  flex-direction: column;\n  color: var(--jp-ui-font-color1);\n}\n.trialProgress h3 {\n  font-weight: bold;\n}\n.trialProgress td {\n  white-space: nowrap;\n}\n</style>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 754/754 [00:38<00:00, 19.35it/s]\n",
      "Epoch 1:   0%|          | 0/754 [00:00<?, ?it/s]training_loss tensor(2.1535, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:   0%|          | 1/754 [00:00<00:37, 19.98it/s]training_loss tensor(2.2141, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:   0%|          | 2/754 [00:00<00:37, 20.25it/s]training_loss tensor(2.1459, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:   0%|          | 3/754 [00:00<00:37, 19.96it/s]training_loss tensor(2.2302, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:   1%|          | 4/754 [00:00<00:37, 20.15it/s]training_loss tensor(2.0482, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:   1%|          | 5/754 [00:00<00:39, 18.79it/s]training_loss tensor(1.2593, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:   1%|          | 6/754 [00:00<00:40, 18.58it/s]training_loss tensor(1.1620, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:   1%|          | 7/754 [00:00<00:39, 18.80it/s]training_loss tensor(1.2487, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:   1%|          | 8/754 [00:00<00:39, 19.00it/s]training_loss tensor(1.1275, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:   1%|          | 9/754 [00:00<00:38, 19.15it/s]training_loss tensor(1.0089, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:   1%|▏         | 10/754 [00:00<00:38, 19.15it/s]training_loss tensor(0.9660, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:   1%|▏         | 11/754 [00:00<00:38, 19.29it/s]training_loss tensor(1.0521, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:   2%|▏         | 12/754 [00:00<00:38, 19.40it/s]training_loss tensor(0.9335, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:   2%|▏         | 13/754 [00:00<00:38, 19.47it/s]training_loss tensor(0.7264, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:   2%|▏         | 14/754 [00:00<00:38, 19.41it/s]training_loss tensor(0.7281, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:   2%|▏         | 15/754 [00:00<00:37, 19.50it/s]training_loss tensor(0.8968, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:   2%|▏         | 16/754 [00:00<00:37, 19.54it/s]training_loss tensor(0.9042, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:   2%|▏         | 17/754 [00:00<00:37, 19.58it/s]training_loss tensor(0.9543, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:   2%|▏         | 18/754 [00:00<00:37, 19.59it/s]training_loss tensor(0.9326, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:   3%|▎         | 19/754 [00:00<00:37, 19.64it/s]training_loss tensor(0.9067, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:   3%|▎         | 20/754 [00:01<00:37, 19.65it/s]training_loss tensor(0.8224, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:   3%|▎         | 21/754 [00:01<00:37, 19.65it/s]training_loss tensor(0.6978, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:   3%|▎         | 22/754 [00:01<00:37, 19.62it/s]training_loss tensor(0.6185, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:   3%|▎         | 23/754 [00:01<00:37, 19.62it/s]training_loss tensor(0.6218, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:   3%|▎         | 24/754 [00:01<00:37, 19.62it/s]training_loss tensor(0.6095, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:   3%|▎         | 25/754 [00:01<00:37, 19.62it/s]training_loss tensor(0.6548, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:   3%|▎         | 26/754 [00:01<00:37, 19.52it/s]training_loss tensor(0.7011, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:   4%|▎         | 27/754 [00:01<00:37, 19.47it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.6332, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:   4%|▎         | 28/754 [00:01<00:37, 19.45it/s]training_loss tensor(0.6165, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:   4%|▍         | 29/754 [00:01<00:37, 19.30it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.6219, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:   4%|▍         | 30/754 [00:01<00:37, 19.16it/s]training_loss tensor(0.5231, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:   4%|▍         | 31/754 [00:01<00:37, 19.11it/s]training_loss tensor(0.4251, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:   4%|▍         | 32/754 [00:01<00:37, 19.06it/s]training_loss tensor(0.3968, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:   4%|▍         | 33/754 [00:01<00:37, 19.02it/s]training_loss tensor(0.2862, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:   5%|▍         | 34/754 [00:01<00:38, 18.94it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.3642, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:   5%|▍         | 35/754 [00:01<00:38, 18.90it/s]training_loss tensor(0.3703, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:   5%|▍         | 36/754 [00:01<00:38, 18.88it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.3670, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:   5%|▍         | 37/754 [00:01<00:38, 18.85it/s]training_loss tensor(0.3668, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:   5%|▌         | 38/754 [00:02<00:38, 18.84it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.4537, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:   5%|▌         | 39/754 [00:02<00:37, 18.86it/s]training_loss tensor(0.4328, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:   5%|▌         | 40/754 [00:02<00:37, 18.89it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.4504, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:   5%|▌         | 41/754 [00:02<00:37, 18.90it/s]training_loss tensor(0.5119, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:   6%|▌         | 42/754 [00:02<00:37, 18.89it/s]training_loss tensor(0.5404, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:   6%|▌         | 43/754 [00:02<00:37, 18.91it/s]training_loss tensor(0.4958, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:   6%|▌         | 44/754 [00:02<00:37, 18.92it/s]training_loss tensor(0.6766, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:   6%|▌         | 45/754 [00:02<00:37, 18.92it/s]training_loss tensor(0.7525, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:   6%|▌         | 46/754 [00:02<00:37, 18.95it/s]training_loss tensor(0.7295, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:   6%|▌         | 47/754 [00:02<00:37, 18.98it/s]training_loss tensor(0.7681, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:   6%|▋         | 48/754 [00:02<00:37, 19.01it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.7291, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:   6%|▋         | 49/754 [00:02<00:37, 19.01it/s]training_loss tensor(0.5775, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:   7%|▋         | 50/754 [00:02<00:36, 19.04it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.4023, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:   7%|▋         | 51/754 [00:02<00:36, 19.03it/s]training_loss tensor(0.5246, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:   7%|▋         | 52/754 [00:02<00:36, 19.02it/s]training_loss tensor(0.5578, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:   7%|▋         | 53/754 [00:02<00:36, 19.01it/s]training_loss tensor(0.9773, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:   7%|▋         | 54/754 [00:02<00:36, 19.00it/s]training_loss tensor(1.3475, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:   7%|▋         | 55/754 [00:02<00:36, 19.01it/s]training_loss tensor(1.4360, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:   7%|▋         | 56/754 [00:02<00:36, 19.03it/s]training_loss tensor(1.4024, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:   8%|▊         | 57/754 [00:02<00:36, 19.02it/s]training_loss tensor(1.4116, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:   8%|▊         | 58/754 [00:03<00:36, 19.03it/s]training_loss tensor(1.2510, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:   8%|▊         | 59/754 [00:03<00:36, 19.04it/s]training_loss tensor(0.6556, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:   8%|▊         | 60/754 [00:03<00:36, 19.07it/s]training_loss tensor(0.5238, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:   8%|▊         | 61/754 [00:03<00:36, 19.05it/s]training_loss tensor(0.3861, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:   8%|▊         | 62/754 [00:03<00:36, 19.05it/s]training_loss tensor(0.3784, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:   8%|▊         | 63/754 [00:03<00:36, 19.06it/s]training_loss tensor(0.3542, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:   8%|▊         | 64/754 [00:03<00:36, 19.07it/s]training_loss tensor(0.3716, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:   9%|▊         | 65/754 [00:03<00:36, 19.09it/s]training_loss tensor(0.2924, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:   9%|▉         | 66/754 [00:03<00:36, 19.11it/s]training_loss tensor(0.3336, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:   9%|▉         | 67/754 [00:03<00:35, 19.12it/s]training_loss tensor(0.3053, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:   9%|▉         | 68/754 [00:03<00:35, 19.15it/s]training_loss tensor(0.3679, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:   9%|▉         | 69/754 [00:03<00:35, 19.15it/s]training_loss tensor(0.4183, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:   9%|▉         | 70/754 [00:03<00:35, 19.15it/s]training_loss tensor(0.4805, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:   9%|▉         | 71/754 [00:03<00:35, 19.16it/s]training_loss tensor(0.5301, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  10%|▉         | 72/754 [00:03<00:35, 19.15it/s]training_loss tensor(0.8291, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  10%|▉         | 73/754 [00:03<00:35, 19.15it/s]training_loss tensor(1.1317, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  10%|▉         | 74/754 [00:03<00:35, 19.16it/s]training_loss tensor(1.2830, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  10%|▉         | 75/754 [00:03<00:35, 19.16it/s]training_loss tensor(1.5070, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  10%|█         | 76/754 [00:03<00:35, 19.18it/s]training_loss tensor(1.5030, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  10%|█         | 77/754 [00:04<00:35, 19.18it/s]training_loss tensor(1.4183, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  10%|█         | 78/754 [00:04<00:35, 19.19it/s]training_loss tensor(1.1656, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  10%|█         | 79/754 [00:04<00:35, 19.21it/s]training_loss tensor(1.1141, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  11%|█         | 80/754 [00:04<00:35, 19.22it/s]training_loss tensor(0.7465, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  11%|█         | 81/754 [00:04<00:35, 19.22it/s]training_loss tensor(0.7594, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  11%|█         | 82/754 [00:04<00:35, 19.20it/s]training_loss tensor(0.8589, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  11%|█         | 83/754 [00:04<00:34, 19.21it/s]training_loss tensor(0.9526, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  11%|█         | 84/754 [00:04<00:34, 19.20it/s]training_loss tensor(0.8653, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  11%|█▏        | 85/754 [00:04<00:34, 19.18it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.8384, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  11%|█▏        | 86/754 [00:04<00:34, 19.15it/s]training_loss tensor(0.9089, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  12%|█▏        | 87/754 [00:04<00:34, 19.15it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.8038, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  12%|█▏        | 88/754 [00:04<00:34, 19.13it/s]training_loss tensor(0.6931, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  12%|█▏        | 89/754 [00:04<00:34, 19.13it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.7384, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  12%|█▏        | 90/754 [00:04<00:34, 19.13it/s]training_loss tensor(0.7225, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  12%|█▏        | 91/754 [00:04<00:34, 19.14it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.1530, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  12%|█▏        | 92/754 [00:04<00:34, 19.12it/s]training_loss tensor(1.2682, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  12%|█▏        | 93/754 [00:04<00:34, 19.12it/s]training_loss tensor(1.3396, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  12%|█▏        | 94/754 [00:04<00:34, 19.12it/s]training_loss tensor(1.3311, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  13%|█▎        | 95/754 [00:04<00:34, 19.12it/s]training_loss tensor(1.3052, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  13%|█▎        | 96/754 [00:05<00:34, 19.12it/s]training_loss tensor(1.0386, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  13%|█▎        | 97/754 [00:05<00:34, 19.13it/s]training_loss tensor(0.7913, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  13%|█▎        | 98/754 [00:05<00:34, 19.13it/s]training_loss tensor(0.5514, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  13%|█▎        | 99/754 [00:05<00:34, 19.15it/s]training_loss tensor(0.5231, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  13%|█▎        | 100/754 [00:05<00:34, 19.15it/s]training_loss tensor(0.5435, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  13%|█▎        | 101/754 [00:05<00:34, 19.15it/s]training_loss tensor(0.5263, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  14%|█▎        | 102/754 [00:05<00:34, 19.15it/s]training_loss tensor(0.4963, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  14%|█▎        | 103/754 [00:05<00:33, 19.16it/s]training_loss tensor(0.5319, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  14%|█▍        | 104/754 [00:05<00:33, 19.17it/s]training_loss tensor(0.5020, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  14%|█▍        | 105/754 [00:05<00:33, 19.16it/s]training_loss tensor(0.4779, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  14%|█▍        | 106/754 [00:05<00:33, 19.15it/s]training_loss tensor(0.3741, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  14%|█▍        | 107/754 [00:05<00:33, 19.13it/s]training_loss tensor(0.4213, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  14%|█▍        | 108/754 [00:05<00:33, 19.11it/s]training_loss tensor(0.3407, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  14%|█▍        | 109/754 [00:05<00:33, 19.11it/s]training_loss tensor(0.3227, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  15%|█▍        | 110/754 [00:05<00:33, 19.08it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.2980, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  15%|█▍        | 111/754 [00:05<00:33, 19.07it/s]training_loss tensor(0.2692, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  15%|█▍        | 112/754 [00:05<00:33, 19.07it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.2444, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  15%|█▍        | 113/754 [00:05<00:33, 19.05it/s]training_loss tensor(0.2144, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  15%|█▌        | 114/754 [00:05<00:33, 19.03it/s]training_loss tensor(0.1993, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  15%|█▌        | 115/754 [00:06<00:33, 19.02it/s]training_loss tensor(0.2200, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  15%|█▌        | 116/754 [00:06<00:33, 19.02it/s]training_loss tensor(0.3074, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  16%|█▌        | 117/754 [00:06<00:33, 19.02it/s]training_loss tensor(0.3744, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  16%|█▌        | 118/754 [00:06<00:33, 19.02it/s]training_loss tensor(0.3898, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  16%|█▌        | 119/754 [00:06<00:33, 19.01it/s]training_loss tensor(0.4197, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  16%|█▌        | 120/754 [00:06<00:33, 19.02it/s]training_loss tensor(0.4530, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  16%|█▌        | 121/754 [00:06<00:33, 19.02it/s]training_loss tensor(0.4240, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  16%|█▌        | 122/754 [00:06<00:33, 19.02it/s]training_loss tensor(0.3588, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  16%|█▋        | 123/754 [00:06<00:33, 19.00it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.3397, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  16%|█▋        | 124/754 [00:06<00:33, 18.97it/s]training_loss tensor(0.5878, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  17%|█▋        | 125/754 [00:06<00:33, 18.95it/s]training_loss tensor(0.7638, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  17%|█▋        | 126/754 [00:06<00:33, 18.95it/s]training_loss tensor(0.7999, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  17%|█▋        | 127/754 [00:06<00:33, 18.94it/s]training_loss tensor(0.8041, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  17%|█▋        | 128/754 [00:06<00:33, 18.92it/s]training_loss tensor(0.9902, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  17%|█▋        | 129/754 [00:06<00:33, 18.91it/s]training_loss tensor(1.1071, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  17%|█▋        | 130/754 [00:06<00:32, 18.91it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.1309, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  17%|█▋        | 131/754 [00:06<00:32, 18.90it/s]training_loss tensor(1.3716, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  18%|█▊        | 132/754 [00:06<00:32, 18.89it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.4429, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  18%|█▊        | 133/754 [00:07<00:32, 18.89it/s]training_loss tensor(1.3974, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  18%|█▊        | 134/754 [00:07<00:32, 18.90it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.3328, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  18%|█▊        | 135/754 [00:07<00:32, 18.90it/s]training_loss tensor(1.2449, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  18%|█▊        | 136/754 [00:07<00:32, 18.89it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.9142, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  18%|█▊        | 137/754 [00:07<00:32, 18.89it/s]training_loss tensor(0.7795, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  18%|█▊        | 138/754 [00:07<00:32, 18.90it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.6583, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  18%|█▊        | 139/754 [00:07<00:32, 18.91it/s]training_loss tensor(0.4887, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  19%|█▊        | 140/754 [00:07<00:32, 18.92it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.3831, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  19%|█▊        | 141/754 [00:07<00:32, 18.93it/s]training_loss tensor(0.7738, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  19%|█▉        | 142/754 [00:07<00:32, 18.93it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.7783, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  19%|█▉        | 143/754 [00:07<00:32, 18.93it/s]training_loss tensor(0.7533, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  19%|█▉        | 144/754 [00:07<00:32, 18.94it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.7477, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  19%|█▉        | 145/754 [00:07<00:32, 18.94it/s]training_loss tensor(0.7507, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  19%|█▉        | 146/754 [00:07<00:32, 18.95it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.4424, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  19%|█▉        | 147/754 [00:07<00:32, 18.93it/s]training_loss tensor(0.2890, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  20%|█▉        | 148/754 [00:07<00:32, 18.93it/s]training_loss tensor(0.2955, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  20%|█▉        | 149/754 [00:07<00:31, 18.93it/s]training_loss tensor(0.3311, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  20%|█▉        | 150/754 [00:07<00:31, 18.94it/s]training_loss tensor(0.3053, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  20%|██        | 151/754 [00:07<00:31, 18.95it/s]training_loss tensor(0.3516, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  20%|██        | 152/754 [00:08<00:31, 18.96it/s]\n",
      "Epoch 1:  20%|██        | 152/754 [00:08<00:31, 18.96it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.3657, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  20%|██        | 153/754 [00:08<00:31, 18.97it/s]training_loss tensor(0.3612, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  20%|██        | 154/754 [00:08<00:31, 18.98it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.2988, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  21%|██        | 155/754 [00:08<00:31, 18.99it/s]training_loss tensor(0.2683, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  21%|██        | 156/754 [00:08<00:31, 18.99it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.2042, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  21%|██        | 157/754 [00:08<00:31, 19.00it/s]training_loss tensor(0.2286, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  21%|██        | 158/754 [00:08<00:31, 19.00it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.2262, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  21%|██        | 159/754 [00:08<00:31, 19.00it/s]training_loss tensor(0.2768, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  21%|██        | 160/754 [00:08<00:31, 19.01it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.2619, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  21%|██▏       | 161/754 [00:08<00:31, 19.02it/s]training_loss tensor(0.3319, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  21%|██▏       | 162/754 [00:08<00:31, 19.02it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.3745, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  22%|██▏       | 163/754 [00:08<00:31, 19.03it/s]training_loss tensor(0.4500, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  22%|██▏       | 164/754 [00:08<00:30, 19.04it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.6000, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  22%|██▏       | 165/754 [00:08<00:30, 19.04it/s]training_loss tensor(0.6518, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  22%|██▏       | 166/754 [00:08<00:30, 19.05it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.6459, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  22%|██▏       | 167/754 [00:08<00:30, 19.05it/s]training_loss tensor(0.6380, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  22%|██▏       | 168/754 [00:08<00:30, 19.06it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.6434, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  22%|██▏       | 169/754 [00:08<00:30, 19.07it/s]training_loss tensor(0.4954, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  23%|██▎       | 170/754 [00:08<00:30, 19.07it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.4217, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  23%|██▎       | 171/754 [00:08<00:30, 19.08it/s]training_loss tensor(0.3702, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  23%|██▎       | 172/754 [00:09<00:30, 19.08it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.4149, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  23%|██▎       | 173/754 [00:09<00:30, 19.08it/s]training_loss tensor(0.3390, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  23%|██▎       | 174/754 [00:09<00:30, 19.09it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.3192, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  23%|██▎       | 175/754 [00:09<00:30, 19.10it/s]training_loss tensor(0.3052, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  23%|██▎       | 176/754 [00:09<00:30, 19.10it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.4063, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  23%|██▎       | 177/754 [00:09<00:30, 19.10it/s]training_loss tensor(0.5467, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  24%|██▎       | 178/754 [00:09<00:30, 19.11it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.5851, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  24%|██▎       | 179/754 [00:09<00:30, 19.11it/s]training_loss tensor(0.6967, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  24%|██▍       | 180/754 [00:09<00:30, 19.12it/s]training_loss tensor(1.2825, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  24%|██▍       | 181/754 [00:09<00:29, 19.12it/s]training_loss tensor(1.4892, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  24%|██▍       | 182/754 [00:09<00:29, 19.12it/s]training_loss tensor(1.5594, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  24%|██▍       | 183/754 [00:09<00:29, 19.13it/s]training_loss tensor(1.5830, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  24%|██▍       | 184/754 [00:09<00:29, 19.13it/s]training_loss tensor(1.5766, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  25%|██▍       | 185/754 [00:09<00:29, 19.14it/s]training_loss tensor(1.2484, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  25%|██▍       | 186/754 [00:09<00:29, 19.14it/s]training_loss tensor(1.0291, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  25%|██▍       | 187/754 [00:09<00:29, 19.15it/s]training_loss tensor(0.7142, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  25%|██▍       | 188/754 [00:09<00:29, 19.15it/s]training_loss tensor(0.5862, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  25%|██▌       | 189/754 [00:09<00:29, 19.16it/s]training_loss tensor(0.6000, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  25%|██▌       | 190/754 [00:09<00:29, 19.16it/s]training_loss tensor(0.5879, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  25%|██▌       | 191/754 [00:09<00:29, 19.17it/s]training_loss tensor(0.5748, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  25%|██▌       | 192/754 [00:10<00:29, 19.17it/s]training_loss tensor(0.5801, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  26%|██▌       | 193/754 [00:10<00:29, 19.17it/s]training_loss tensor(0.6235, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  26%|██▌       | 194/754 [00:10<00:29, 19.18it/s]training_loss tensor(0.5420, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  26%|██▌       | 195/754 [00:10<00:29, 19.18it/s]training_loss tensor(0.6095, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  26%|██▌       | 196/754 [00:10<00:29, 19.19it/s]training_loss tensor(0.8123, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  26%|██▌       | 197/754 [00:10<00:29, 19.19it/s]training_loss tensor(0.8633, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  26%|██▋       | 198/754 [00:10<00:28, 19.19it/s]training_loss tensor(1.2291, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  26%|██▋       | 199/754 [00:10<00:28, 19.20it/s]training_loss tensor(1.3639, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  27%|██▋       | 200/754 [00:10<00:28, 19.20it/s]training_loss tensor(1.4793, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  27%|██▋       | 201/754 [00:10<00:28, 19.21it/s]training_loss tensor(1.3959, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  27%|██▋       | 202/754 [00:10<00:28, 19.21it/s]training_loss tensor(1.4000, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  27%|██▋       | 203/754 [00:10<00:28, 19.22it/s]training_loss tensor(1.1343, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  27%|██▋       | 204/754 [00:10<00:28, 19.22it/s]training_loss tensor(0.9137, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  27%|██▋       | 205/754 [00:10<00:28, 19.22it/s]training_loss tensor(0.6199, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  27%|██▋       | 206/754 [00:10<00:28, 19.22it/s]training_loss tensor(0.4803, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  27%|██▋       | 207/754 [00:10<00:28, 19.22it/s]training_loss tensor(0.4567, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  28%|██▊       | 208/754 [00:10<00:28, 19.22it/s]training_loss tensor(0.3892, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  28%|██▊       | 209/754 [00:10<00:28, 19.23it/s]training_loss tensor(0.5008, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  28%|██▊       | 210/754 [00:10<00:28, 19.23it/s]training_loss tensor(0.5905, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  28%|██▊       | 211/754 [00:10<00:28, 19.24it/s]training_loss tensor(0.7241, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  28%|██▊       | 212/754 [00:11<00:28, 19.24it/s]training_loss tensor(0.8428, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  28%|██▊       | 213/754 [00:11<00:28, 19.25it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.9431, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  28%|██▊       | 214/754 [00:11<00:28, 19.25it/s]training_loss tensor(1.0028, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  29%|██▊       | 215/754 [00:11<00:27, 19.26it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.2069, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  29%|██▊       | 216/754 [00:11<00:27, 19.26it/s]training_loss tensor(1.2542, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  29%|██▉       | 217/754 [00:11<00:27, 19.24it/s]training_loss tensor(1.2316, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  29%|██▉       | 218/754 [00:11<00:27, 19.23it/s]training_loss tensor(1.2855, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  29%|██▉       | 219/754 [00:11<00:27, 19.22it/s]training_loss tensor(1.3068, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  29%|██▉       | 220/754 [00:11<00:27, 19.22it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.2878, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  29%|██▉       | 221/754 [00:11<00:27, 19.21it/s]training_loss tensor(1.2189, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  29%|██▉       | 222/754 [00:11<00:27, 19.21it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.1602, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  30%|██▉       | 223/754 [00:11<00:27, 19.21it/s]training_loss tensor(1.1445, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  30%|██▉       | 224/754 [00:11<00:27, 19.22it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.0230, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  30%|██▉       | 225/754 [00:11<00:27, 19.22it/s]training_loss tensor(0.7711, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  30%|██▉       | 226/754 [00:11<00:27, 19.22it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.7302, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  30%|███       | 227/754 [00:11<00:27, 19.23it/s]training_loss tensor(0.8121, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  30%|███       | 228/754 [00:11<00:27, 19.23it/s]training_loss tensor(1.1945, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  30%|███       | 229/754 [00:11<00:27, 19.23it/s]training_loss tensor(1.7693, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  31%|███       | 230/754 [00:11<00:27, 19.24it/s]training_loss tensor(1.8283, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  31%|███       | 231/754 [00:12<00:27, 19.24it/s]training_loss tensor(1.8631, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  31%|███       | 232/754 [00:12<00:27, 19.25it/s]training_loss tensor(1.8903, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  31%|███       | 233/754 [00:12<00:27, 19.25it/s]training_loss tensor(1.7957, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  31%|███       | 234/754 [00:12<00:27, 19.25it/s]training_loss tensor(1.3304, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  31%|███       | 235/754 [00:12<00:26, 19.26it/s]training_loss tensor(1.2453, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  31%|███▏      | 236/754 [00:12<00:26, 19.26it/s]training_loss tensor(1.2089, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  31%|███▏      | 237/754 [00:12<00:26, 19.26it/s]training_loss tensor(1.1684, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  32%|███▏      | 238/754 [00:12<00:26, 19.27it/s]training_loss tensor(0.9628, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  32%|███▏      | 239/754 [00:12<00:26, 19.27it/s]training_loss tensor(0.8880, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  32%|███▏      | 240/754 [00:12<00:26, 19.27it/s]training_loss tensor(0.8740, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  32%|███▏      | 241/754 [00:12<00:26, 19.27it/s]training_loss tensor(0.9547, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  32%|███▏      | 242/754 [00:12<00:26, 19.28it/s]training_loss tensor(1.0077, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  32%|███▏      | 243/754 [00:12<00:26, 19.28it/s]training_loss tensor(1.7311, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  32%|███▏      | 244/754 [00:12<00:26, 19.28it/s]training_loss tensor(1.9144, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  32%|███▏      | 245/754 [00:12<00:26, 19.27it/s]training_loss tensor(1.9186, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  33%|███▎      | 246/754 [00:12<00:26, 19.27it/s]training_loss tensor(1.8926, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  33%|███▎      | 247/754 [00:12<00:26, 19.27it/s]training_loss tensor(1.8535, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  33%|███▎      | 248/754 [00:12<00:26, 19.28it/s]training_loss tensor(1.3981, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  33%|███▎      | 249/754 [00:12<00:26, 19.28it/s]training_loss tensor(0.9730, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  33%|███▎      | 250/754 [00:12<00:26, 19.28it/s]training_loss tensor(1.0534, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  33%|███▎      | 251/754 [00:13<00:26, 19.29it/s]training_loss tensor(0.9555, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  33%|███▎      | 252/754 [00:13<00:26, 19.29it/s]training_loss tensor(0.9189, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  34%|███▎      | 253/754 [00:13<00:25, 19.30it/s]training_loss tensor(0.9980, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  34%|███▎      | 254/754 [00:13<00:25, 19.30it/s]training_loss tensor(1.0503, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  34%|███▍      | 255/754 [00:13<00:25, 19.30it/s]training_loss tensor(0.9318, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  34%|███▍      | 256/754 [00:13<00:25, 19.31it/s]training_loss tensor(0.9065, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  34%|███▍      | 257/754 [00:13<00:25, 19.31it/s]training_loss tensor(1.0299, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  34%|███▍      | 258/754 [00:13<00:25, 19.31it/s]training_loss tensor(1.2381, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  34%|███▍      | 259/754 [00:13<00:25, 19.32it/s]training_loss tensor(1.3211, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  34%|███▍      | 260/754 [00:13<00:25, 19.31it/s]training_loss tensor(1.2650, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  35%|███▍      | 261/754 [00:13<00:25, 19.32it/s]training_loss tensor(1.3272, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  35%|███▍      | 262/754 [00:13<00:25, 19.32it/s]training_loss tensor(1.2428, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  35%|███▍      | 263/754 [00:13<00:25, 19.33it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.1955, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  35%|███▌      | 264/754 [00:13<00:25, 19.33it/s]training_loss tensor(1.9034, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  35%|███▌      | 265/754 [00:13<00:25, 19.33it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.9951, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  35%|███▌      | 266/754 [00:13<00:25, 19.33it/s]training_loss tensor(2.0745, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  35%|███▌      | 267/754 [00:13<00:25, 19.34it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(2.1664, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  36%|███▌      | 268/754 [00:13<00:25, 19.34it/s]training_loss tensor(2.3153, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  36%|███▌      | 269/754 [00:13<00:25, 19.34it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.9386, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  36%|███▌      | 270/754 [00:13<00:25, 19.34it/s]training_loss tensor(1.9305, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  36%|███▌      | 271/754 [00:14<00:24, 19.35it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.9282, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  36%|███▌      | 272/754 [00:14<00:24, 19.35it/s]training_loss tensor(1.9683, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  36%|███▌      | 273/754 [00:14<00:24, 19.35it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.8354, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  36%|███▋      | 274/754 [00:14<00:24, 19.35it/s]training_loss tensor(1.9315, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  36%|███▋      | 275/754 [00:14<00:24, 19.35it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.9091, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  37%|███▋      | 276/754 [00:14<00:24, 19.35it/s]training_loss tensor(1.8353, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  37%|███▋      | 277/754 [00:14<00:24, 19.35it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.7688, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  37%|███▋      | 278/754 [00:14<00:24, 19.35it/s]training_loss tensor(1.8236, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  37%|███▋      | 279/754 [00:14<00:24, 19.36it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(2.0191, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  37%|███▋      | 280/754 [00:14<00:24, 19.36it/s]training_loss tensor(1.9019, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  37%|███▋      | 281/754 [00:14<00:24, 19.36it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.9636, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  37%|███▋      | 282/754 [00:14<00:24, 19.37it/s]training_loss tensor(2.0300, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  38%|███▊      | 283/754 [00:14<00:24, 19.37it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.9551, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  38%|███▊      | 284/754 [00:14<00:24, 19.37it/s]training_loss tensor(1.6107, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  38%|███▊      | 285/754 [00:14<00:24, 19.38it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.4423, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  38%|███▊      | 286/754 [00:14<00:24, 19.38it/s]training_loss tensor(1.3090, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  38%|███▊      | 287/754 [00:14<00:24, 19.38it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.2873, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  38%|███▊      | 288/754 [00:14<00:24, 19.38it/s]training_loss tensor(1.3120, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  38%|███▊      | 289/754 [00:14<00:23, 19.38it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.3031, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  38%|███▊      | 290/754 [00:14<00:23, 19.39it/s]training_loss tensor(1.2452, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  39%|███▊      | 291/754 [00:15<00:23, 19.38it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.2187, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  39%|███▊      | 292/754 [00:15<00:23, 19.39it/s]training_loss tensor(1.2512, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  39%|███▉      | 293/754 [00:15<00:23, 19.39it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.1372, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  39%|███▉      | 294/754 [00:15<00:23, 19.39it/s]training_loss tensor(1.1045, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  39%|███▉      | 295/754 [00:15<00:23, 19.40it/s]training_loss tensor(1.1889, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  39%|███▉      | 296/754 [00:15<00:23, 19.39it/s]training_loss tensor(1.2604, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  39%|███▉      | 297/754 [00:15<00:23, 19.39it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.1605, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  40%|███▉      | 298/754 [00:15<00:23, 19.39it/s]training_loss tensor(1.1430, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  40%|███▉      | 299/754 [00:15<00:23, 19.39it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.2225, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  40%|███▉      | 300/754 [00:15<00:23, 19.40it/s]training_loss tensor(1.1586, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  40%|███▉      | 301/754 [00:15<00:23, 19.40it/s]training_loss tensor(1.0134, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  40%|████      | 302/754 [00:15<00:23, 19.40it/s]training_loss tensor(0.9983, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  40%|████      | 303/754 [00:15<00:23, 19.41it/s]training_loss tensor(1.0247, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  40%|████      | 304/754 [00:15<00:23, 19.41it/s]training_loss tensor(0.7746, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  40%|████      | 305/754 [00:15<00:23, 19.41it/s]training_loss tensor(0.7755, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  41%|████      | 306/754 [00:15<00:23, 19.42it/s]training_loss tensor(0.7893, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  41%|████      | 307/754 [00:15<00:23, 19.42it/s]training_loss tensor(0.8257, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  41%|████      | 308/754 [00:15<00:22, 19.42it/s]training_loss tensor(0.7154, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  41%|████      | 309/754 [00:15<00:22, 19.42it/s]training_loss tensor(0.6905, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  41%|████      | 310/754 [00:15<00:22, 19.42it/s]training_loss tensor(0.8267, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  41%|████      | 311/754 [00:16<00:22, 19.42it/s]training_loss tensor(0.9004, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  41%|████▏     | 312/754 [00:16<00:22, 19.43it/s]training_loss tensor(0.8884, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  42%|████▏     | 313/754 [00:16<00:22, 19.43it/s]training_loss tensor(0.9985, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  42%|████▏     | 314/754 [00:16<00:22, 19.43it/s]training_loss tensor(1.0257, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  42%|████▏     | 315/754 [00:16<00:22, 19.43it/s]training_loss tensor(1.0444, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  42%|████▏     | 316/754 [00:16<00:22, 19.44it/s]training_loss tensor(0.8552, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  42%|████▏     | 317/754 [00:16<00:22, 19.43it/s]training_loss tensor(0.8177, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  42%|████▏     | 318/754 [00:16<00:22, 19.43it/s]training_loss tensor(0.7716, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  42%|████▏     | 319/754 [00:16<00:22, 19.42it/s]training_loss tensor(0.7800, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  42%|████▏     | 320/754 [00:16<00:22, 19.42it/s]training_loss tensor(0.8163, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  43%|████▎     | 321/754 [00:16<00:22, 19.42it/s]training_loss tensor(0.8993, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  43%|████▎     | 322/754 [00:16<00:22, 19.42it/s]training_loss tensor(0.9349, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  43%|████▎     | 323/754 [00:16<00:22, 19.42it/s]training_loss tensor(0.9264, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  43%|████▎     | 324/754 [00:16<00:22, 19.43it/s]training_loss tensor(0.9283, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  43%|████▎     | 325/754 [00:16<00:22, 19.43it/s]training_loss tensor(0.9506, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  43%|████▎     | 326/754 [00:16<00:22, 19.43it/s]training_loss tensor(0.9232, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  43%|████▎     | 327/754 [00:16<00:21, 19.43it/s]training_loss tensor(0.8319, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  44%|████▎     | 328/754 [00:16<00:21, 19.43it/s]training_loss tensor(0.8217, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  44%|████▎     | 329/754 [00:16<00:21, 19.43it/s]training_loss tensor(0.7991, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  44%|████▍     | 330/754 [00:16<00:21, 19.43it/s]training_loss tensor(0.6644, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  44%|████▍     | 331/754 [00:17<00:21, 19.44it/s]training_loss tensor(0.5594, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  44%|████▍     | 332/754 [00:17<00:21, 19.44it/s]training_loss tensor(0.7165, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  44%|████▍     | 333/754 [00:17<00:21, 19.44it/s]training_loss tensor(0.7072, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  44%|████▍     | 334/754 [00:17<00:21, 19.45it/s]training_loss tensor(0.6679, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  44%|████▍     | 335/754 [00:17<00:21, 19.45it/s]training_loss tensor(0.6877, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  45%|████▍     | 336/754 [00:17<00:21, 19.45it/s]training_loss tensor(0.6829, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  45%|████▍     | 337/754 [00:17<00:21, 19.45it/s]training_loss tensor(0.5677, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  45%|████▍     | 338/754 [00:17<00:21, 19.45it/s]training_loss tensor(0.4266, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  45%|████▍     | 339/754 [00:17<00:21, 19.45it/s]training_loss tensor(0.4369, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  45%|████▌     | 340/754 [00:17<00:21, 19.46it/s]training_loss tensor(0.3644, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  45%|████▌     | 341/754 [00:17<00:21, 19.46it/s]training_loss tensor(0.4266, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  45%|████▌     | 342/754 [00:17<00:21, 19.46it/s]training_loss tensor(0.4582, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  45%|████▌     | 343/754 [00:17<00:21, 19.46it/s]training_loss tensor(0.4364, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  46%|████▌     | 344/754 [00:17<00:21, 19.45it/s]training_loss tensor(0.4512, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  46%|████▌     | 345/754 [00:17<00:21, 19.45it/s]training_loss tensor(0.4562, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  46%|████▌     | 346/754 [00:17<00:20, 19.45it/s]training_loss tensor(0.3839, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  46%|████▌     | 347/754 [00:17<00:20, 19.45it/s]training_loss tensor(0.2474, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  46%|████▌     | 348/754 [00:17<00:20, 19.45it/s]training_loss tensor(0.2282, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  46%|████▋     | 349/754 [00:17<00:20, 19.45it/s]training_loss tensor(0.4796, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  46%|████▋     | 350/754 [00:17<00:20, 19.46it/s]training_loss tensor(0.7636, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  47%|████▋     | 351/754 [00:18<00:20, 19.46it/s]training_loss tensor(1.2029, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  47%|████▋     | 352/754 [00:18<00:20, 19.46it/s]training_loss tensor(1.4196, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  47%|████▋     | 353/754 [00:18<00:20, 19.46it/s]training_loss tensor(1.6288, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  47%|████▋     | 354/754 [00:18<00:20, 19.46it/s]training_loss tensor(1.6460, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  47%|████▋     | 355/754 [00:18<00:20, 19.46it/s]training_loss tensor(1.6375, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  47%|████▋     | 356/754 [00:18<00:20, 19.46it/s]training_loss tensor(1.3646, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  47%|████▋     | 357/754 [00:18<00:20, 19.46it/s]training_loss tensor(1.2236, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  47%|████▋     | 358/754 [00:18<00:20, 19.46it/s]training_loss tensor(1.1659, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  48%|████▊     | 359/754 [00:18<00:20, 19.47it/s]training_loss tensor(1.1625, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  48%|████▊     | 360/754 [00:18<00:20, 19.47it/s]training_loss tensor(1.1796, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  48%|████▊     | 361/754 [00:18<00:20, 19.47it/s]training_loss tensor(1.1734, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  48%|████▊     | 362/754 [00:18<00:20, 19.47it/s]training_loss tensor(1.1320, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  48%|████▊     | 363/754 [00:18<00:20, 19.47it/s]training_loss tensor(1.0606, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  48%|████▊     | 364/754 [00:18<00:20, 19.47it/s]training_loss tensor(0.9796, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  48%|████▊     | 365/754 [00:18<00:19, 19.47it/s]training_loss tensor(0.9037, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  49%|████▊     | 366/754 [00:18<00:19, 19.48it/s]training_loss tensor(0.8163, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  49%|████▊     | 367/754 [00:18<00:19, 19.48it/s]training_loss tensor(0.7545, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  49%|████▉     | 368/754 [00:18<00:19, 19.48it/s]training_loss tensor(0.7151, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  49%|████▉     | 369/754 [00:18<00:19, 19.48it/s]training_loss tensor(0.4619, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  49%|████▉     | 370/754 [00:18<00:19, 19.48it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.3842, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  49%|████▉     | 371/754 [00:19<00:19, 19.48it/s]training_loss tensor(0.5460, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  49%|████▉     | 372/754 [00:19<00:19, 19.48it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.5008, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  49%|████▉     | 373/754 [00:19<00:19, 19.48it/s]training_loss tensor(0.5800, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  50%|████▉     | 374/754 [00:19<00:19, 19.48it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.6201, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  50%|████▉     | 375/754 [00:19<00:19, 19.48it/s]training_loss tensor(0.7890, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  50%|████▉     | 376/754 [00:19<00:19, 19.49it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.7354, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  50%|█████     | 377/754 [00:19<00:19, 19.49it/s]training_loss tensor(0.7045, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  50%|█████     | 378/754 [00:19<00:19, 19.49it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.6745, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  50%|█████     | 379/754 [00:19<00:19, 19.49it/s]training_loss tensor(0.6219, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  50%|█████     | 380/754 [00:19<00:19, 19.49it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.4197, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  51%|█████     | 381/754 [00:19<00:19, 19.50it/s]training_loss tensor(0.3231, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  51%|█████     | 382/754 [00:19<00:19, 19.50it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.3330, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  51%|█████     | 383/754 [00:19<00:19, 19.50it/s]training_loss tensor(0.2906, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  51%|█████     | 384/754 [00:19<00:18, 19.50it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.8454, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  51%|█████     | 385/754 [00:19<00:18, 19.50it/s]training_loss tensor(0.9553, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  51%|█████     | 386/754 [00:19<00:18, 19.50it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.0015, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  51%|█████▏    | 387/754 [00:19<00:18, 19.50it/s]training_loss tensor(1.0083, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  51%|█████▏    | 388/754 [00:19<00:18, 19.50it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.0404, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  52%|█████▏    | 389/754 [00:19<00:18, 19.51it/s]training_loss tensor(0.8263, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  52%|█████▏    | 390/754 [00:19<00:18, 19.51it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.5677, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  52%|█████▏    | 391/754 [00:20<00:18, 19.51it/s]training_loss tensor(0.5536, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  52%|█████▏    | 392/754 [00:20<00:18, 19.51it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.5353, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  52%|█████▏    | 393/754 [00:20<00:18, 19.51it/s]training_loss tensor(0.5722, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  52%|█████▏    | 394/754 [00:20<00:18, 19.51it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss \n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m tensor(1.0125, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  52%|█████▏    | 395/754 [00:20<00:18, 19.51it/s]training_loss tensor(1.1271, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  53%|█████▎    | 396/754 [00:20<00:18, 19.52it/s]training_loss tensor(1.3714, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  53%|█████▎    | 397/754 [00:20<00:18, 19.52it/s]training_loss tensor(1.3737, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  53%|█████▎    | 398/754 [00:20<00:18, 19.52it/s]training_loss tensor(1.4156, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  53%|█████▎    | 399/754 [00:20<00:18, 19.52it/s]training_loss tensor(1.2982, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  53%|█████▎    | 400/754 [00:20<00:18, 19.52it/s]training_loss tensor(1.1546, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  53%|█████▎    | 401/754 [00:20<00:18, 19.52it/s]training_loss tensor(0.9239, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  53%|█████▎    | 402/754 [00:20<00:18, 19.53it/s]training_loss tensor(0.8660, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  53%|█████▎    | 403/754 [00:20<00:17, 19.53it/s]training_loss tensor(0.8134, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  54%|█████▎    | 404/754 [00:20<00:17, 19.53it/s]training_loss tensor(0.8928, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  54%|█████▎    | 405/754 [00:20<00:17, 19.53it/s]training_loss tensor(1.0378, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  54%|█████▍    | 406/754 [00:20<00:17, 19.53it/s]training_loss tensor(1.5206, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  54%|█████▍    | 407/754 [00:20<00:17, 19.53it/s]training_loss tensor(1.6586, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  54%|█████▍    | 408/754 [00:20<00:17, 19.53it/s]training_loss tensor(1.6885, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  54%|█████▍    | 409/754 [00:20<00:17, 19.53it/s]training_loss tensor(1.7379, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  54%|█████▍    | 410/754 [00:20<00:17, 19.53it/s]training_loss tensor(1.8758, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  55%|█████▍    | 411/754 [00:21<00:17, 19.54it/s]training_loss tensor(1.6690, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  55%|█████▍    | 412/754 [00:21<00:17, 19.54it/s]training_loss tensor(1.5090, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  55%|█████▍    | 413/754 [00:21<00:17, 19.54it/s]training_loss tensor(1.4367, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  55%|█████▍    | 414/754 [00:21<00:17, 19.54it/s]training_loss tensor(1.3863, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  55%|█████▌    | 415/754 [00:21<00:17, 19.54it/s]training_loss tensor(1.1619, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  55%|█████▌    | 416/754 [00:21<00:17, 19.54it/s]training_loss tensor(0.8050, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  55%|█████▌    | 417/754 [00:21<00:17, 19.54it/s]training_loss tensor(0.8460, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  55%|█████▌    | 418/754 [00:21<00:17, 19.53it/s]training_loss tensor(0.9093, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  56%|█████▌    | 419/754 [00:21<00:17, 19.53it/s]training_loss tensor(0.9263, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  56%|█████▌    | 420/754 [00:21<00:17, 19.52it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.9557, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  56%|█████▌    | 421/754 [00:21<00:17, 19.52it/s]training_loss tensor(0.8709, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  56%|█████▌    | 422/754 [00:21<00:17, 19.51it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.8665, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  56%|█████▌    | 423/754 [00:21<00:16, 19.51it/s]training_loss tensor(0.7462, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  56%|█████▌    | 424/754 [00:21<00:16, 19.52it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.6905, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  56%|█████▋    | 425/754 [00:21<00:16, 19.52it/s]training_loss tensor(0.6018, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  56%|█████▋    | 426/754 [00:21<00:16, 19.52it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.5648, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  57%|█████▋    | 427/754 [00:21<00:16, 19.52it/s]training_loss tensor(0.4560, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  57%|█████▋    | 428/754 [00:21<00:16, 19.52it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.8043, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  57%|█████▋    | 429/754 [00:21<00:16, 19.52it/s]training_loss tensor(0.8534, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  57%|█████▋    | 430/754 [00:22<00:16, 19.52it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.8475, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  57%|█████▋    | 431/754 [00:22<00:16, 19.52it/s]training_loss tensor(0.8682, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  57%|█████▋    | 432/754 [00:22<00:16, 19.52it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.9077, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  57%|█████▋    | 433/754 [00:22<00:16, 19.53it/s]training_loss tensor(0.6486, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  58%|█████▊    | 434/754 [00:22<00:16, 19.53it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.4722, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  58%|█████▊    | 435/754 [00:22<00:16, 19.53it/s]training_loss tensor(0.7513, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  58%|█████▊    | 436/754 [00:22<00:16, 19.53it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(1.1184, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  58%|█████▊    | 437/754 [00:22<00:16, 19.53it/s]training_loss tensor(1.1032, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  58%|█████▊    | 438/754 [00:22<00:16, 19.53it/s]training_loss \n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m tensor(1.0867, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  58%|█████▊    | 439/754 [00:22<00:16, 19.53it/s]training_loss tensor(1.1016, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  58%|█████▊    | 440/754 [00:22<00:16, 19.53it/s]training_loss tensor(1.0872, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  58%|█████▊    | 441/754 [00:22<00:16, 19.54it/s]training_loss tensor(0.5841, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  59%|█████▊    | 442/754 [00:22<00:15, 19.54it/s]training_loss tensor(0.6112, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  59%|█████▉    | 443/754 [00:22<00:15, 19.53it/s]training_loss tensor(0.7246, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  59%|█████▉    | 444/754 [00:22<00:15, 19.53it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.7450, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  59%|█████▉    | 445/754 [00:22<00:15, 19.53it/s]training_loss tensor(0.7337, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  59%|█████▉    | 446/754 [00:22<00:15, 19.53it/s]\n",
      "\u001B[2m\u001B[36m(train_transformer pid=11571)\u001B[0m training_loss tensor(0.7450, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  59%|█████▉    | 445/754 [00:22<00:15, 19.53it/s]training_loss tensor(0.7337, grad_fn=<SqrtBackward0>)\n",
      "Epoch 1:  59%|█████▉    | 446/754 [00:22<00:15, 19.53it/s]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "%load_ext tensorboard"
   ],
   "metadata": {
    "id": "GPiQbeM1BN91",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "dd8caee3-589f-493d-9da4-5db33ba0c58f"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "%tensorboard --logdir=\"/content/drive/MyDrive/models/transformers/checkpoints/ta_corr_with_logg_diffed\""
   ],
   "metadata": {
    "id": "9_l_kZnRDfGs",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "outputId": "fc08abf9-723e-4dc6-d8a8-760c346613c6"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "ERROR: Failed to launch TensorBoard (exited with 1).\n",
       "Contents of stderr:\n",
       "2023-05-30 19:57:36.512598: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
       "2023-05-30 19:57:36.567574: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
       "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
       "2023-05-30 19:57:37.483972: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
       "/usr/local/lib/python3.10/dist-packages/tensorboard_data_server/bin/server: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.33' not found (required by /usr/local/lib/python3.10/dist-packages/tensorboard_data_server/bin/server)\n",
       "/usr/local/lib/python3.10/dist-packages/tensorboard_data_server/bin/server: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.34' not found (required by /usr/local/lib/python3.10/dist-packages/tensorboard_data_server/bin/server)\n",
       "/usr/local/lib/python3.10/dist-packages/tensorboard_data_server/bin/server: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.32' not found (required by /usr/local/lib/python3.10/dist-packages/tensorboard_data_server/bin/server)\n",
       "Address already in use\n",
       "Port 6006 is in use by another program. Either identify and stop that program, or start the server with a different port."
      ]
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "analysis."
   ],
   "metadata": {
    "id": "RYimsjSEDlJ-"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%cp -av \"/root/ray_results/tune_transformer\" \"/content/drive/MyDrive/models/transformers/checkpoints/logged_diffed_timestep30/ray_logs\""
   ],
   "metadata": {
    "id": "8PAZzFvNR8LR"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "analysis.get_best_config()"
   ],
   "metadata": {
    "id": "_ZT2bthr7fHy"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "analysis.best_checkpoint.path"
   ],
   "metadata": {
    "id": "OlBsShaQD2Hu",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "outputId": "e539c8dd-3b95-454c-f348-f9a3f42f6906"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/content/drive/MyDrive/models/transformers/checkpoints/ta_corr_with_logg_diffed/tune_transformer/train_transformer_f6d79_00001_1_attn_type=dense,batch_size=4,dropout=0.2000,learning_rate=0.0000,n_epochs=70,nhead=4,num_layers=4,_2023-05-30_19-45-55/checkpoint_epoch=33-step=25942'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 34
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "model = MyTransformer.load_from_checkpoint(analysis.best_checkpoint.path + '/checkpoint',  loss_fn=RMSELoss)\n",
    "trainer=pl.Trainer()"
   ],
   "metadata": {
    "id": "Wq0_l7dnLtYI"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "best_config = analysis.get_best_config()\n",
    "\n",
    "Xtrain, Ytrain, Xtest, Ytest, XVal, YVal = make_dataset(\n",
    "    df, target_col='log_returns', exclude_cols=['RSI-based MA'], timestep=10, ntest=21\n",
    ")\n",
    "train_loader, val_loader, test_loader, test_loader_one = get_torch_data_loaders(\n",
    "    Xtrain, Ytrain, Xtest, Ytest, XVal, YVal, best_config['batch_size']\n",
    ")"
   ],
   "metadata": {
    "id": "44YqMan37Omj"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "trainer.test(model, test_loader_one)"
   ],
   "metadata": {
    "id": "GNYJYuwa7uqb"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "context = ray.init()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "gpuType": "A100",
   "machine_shape": "hm"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
