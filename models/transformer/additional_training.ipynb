{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-06-04T18:44:12.290200800Z",
     "start_time": "2023-06-04T18:44:06.988421900Z"
    }
   },
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.loggers import MLFlowLogger\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "import torch\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from ray.tune.search import ConcurrencyLimiter\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from ray.util.client import ray\n",
    "\n",
    "\n",
    "import sys\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.loggers import MLFlowLogger\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "#from pytorch_lightning.callbacks import TQDMProgressBar\n",
    "import numpy as np\n",
    "import tempfile\n",
    "from ray import tune\n",
    "from ray.tune.integration.pytorch_lightning import TuneReportCallback, TuneReportCheckpointCallback\n",
    "\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from ray.air.config import RunConfig, ScalingConfig, CheckpointConfig\n",
    "from ray.train.lightning import (\n",
    "    LightningTrainer,\n",
    "    LightningConfigBuilder,\n",
    "    LightningCheckpoint,\n",
    ")\n",
    "\n",
    "\n",
    "from models.transformer.kw_transformer import TransAm\n",
    "from kw_transformer_functions import RMSELoss\n",
    "from models.transformer.my_functrions import make_dataset, get_torch_data_loaders\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cpu')"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T18:44:12.304723800Z",
     "start_time": "2023-06-04T18:44:12.292197800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "            log_close        Close  log_returns  diff_fear_greed  \\\ntimestamp                                                          \n2018-01-19   6.946110  1039.099976     0.002718        -0.008588   \n2018-01-20   7.051986  1155.150024     0.105875         0.094570   \n2018-01-21   6.956145  1049.579956    -0.095840        -0.107145   \n2018-01-22   6.911010  1003.260010    -0.045135        -0.056441   \n2018-01-23   6.893889   986.229004    -0.017121        -0.028427   \n...               ...          ...          ...              ...   \n2023-05-31   7.535900  1874.130493    -0.014249        -1.014249   \n2023-06-01   7.529515  1862.201416    -0.006385         1.993615   \n2023-06-02   7.553421  1907.256592     0.023907        -2.976093   \n2023-06-03   7.545608  1892.412476    -0.007813         0.992187   \n2023-06-04   7.550950  1902.548706     0.005342        -0.005963   \n\n            volatility_kcp   volume_em    volume_vpt  momentum_stoch_rsi  \\\ntimestamp                                                                  \n2018-01-19        0.102478   31.624126  1.400980e+08            0.037616   \n2018-01-20        0.454029  176.900699  4.550768e+08            0.209534   \n2018-01-21        0.189903  -69.272645  1.352358e+08            0.033644   \n2018-01-22        0.117475 -326.973658 -4.768732e+08            0.000000   \n2018-01-23        0.169233 -109.817190 -2.285243e+08            0.000000   \n...                    ...         ...           ...                 ...   \n2023-05-31        0.765721  -18.994183 -6.215059e+07            0.627877   \n2023-06-01        0.590639   -9.265335 -1.205696e+08            0.493067   \n2023-06-02        0.989787   13.536856  1.116329e+08            0.865498   \n2023-06-03        0.790237   10.775132  1.205078e+08            0.714422   \n2023-06-04        0.806855    0.219065 -8.870213e+06            0.791131   \n\n            volatility_bbp  momentum_stoch  ...  trend_macd_diff  trend_stc  \\\ntimestamp                                   ...                               \n2018-01-19        0.422813       39.600400  ...       -36.805177   0.000000   \n2018-01-20        0.564169       57.400634  ...       -32.297077   0.000000   \n2018-01-21        0.361546       41.207862  ...       -35.591281   0.000000   \n2018-01-22        0.255003       34.103119  ...       -39.740806   0.000000   \n2018-01-23        0.217811       31.490833  ...       -42.249176   0.000000   \n...                    ...             ...  ...              ...        ...   \n2023-05-31        0.807676       67.930616  ...        11.160775  94.726423   \n2023-06-01        0.701580       60.614706  ...         8.930618  97.363212   \n2023-06-02        0.953481       88.246317  ...         9.968121  98.681606   \n2023-06-03        0.818537       79.142660  ...         9.153992  99.340803   \n2023-06-04        0.843562       85.359047  ...         8.795296  99.670401   \n\n            volume_sma_em  trend_aroon_ind  momentum_tsi  trend_aroon_down  \\\ntimestamp                                                                    \n2018-01-19     -60.164463             56.0     21.811785              20.0   \n2018-01-20     -48.724324             56.0     20.500950              16.0   \n2018-01-21     -63.621180             56.0     17.246075              12.0   \n2018-01-22     -96.179267             56.0     13.818018               8.0   \n2018-01-23    -118.460975             56.0     10.820051               4.0   \n...                   ...              ...           ...               ...   \n2023-05-31       4.602985             60.0      2.887780              28.0   \n2023-06-01       4.538830             60.0      2.991828              24.0   \n2023-06-02       5.120619             60.0      4.968846              20.0   \n2023-06-03       5.549683             60.0      5.860393              16.0   \n2023-06-04       5.836365             60.0      6.994869              12.0   \n\n            trend_vortex_ind_neg  trend_psar_down_indicator  trend_adx_neg  \\\ntimestamp                                                                    \n2018-01-19              0.992330                        0.0      26.633272   \n2018-01-20              0.955632                        0.0      24.958464   \n2018-01-21              0.996763                        0.0      24.462554   \n2018-01-22              1.069065                        0.0      27.014842   \n2018-01-23              1.120754                        0.0      26.135747   \n...                          ...                        ...            ...   \n2023-05-31              0.886572                        0.0      18.486077   \n2023-06-01              0.908415                        0.0      18.237269   \n2023-06-02              0.869446                        0.0      16.780865   \n2023-06-03              0.876527                        0.0      16.222529   \n2023-06-04              0.873427                        0.0      15.660544   \n\n            volatility_bbli  \ntimestamp                    \n2018-01-19              0.0  \n2018-01-20              0.0  \n2018-01-21              0.0  \n2018-01-22              0.0  \n2018-01-23              0.0  \n...                     ...  \n2023-05-31              0.0  \n2023-06-01              0.0  \n2023-06-02              0.0  \n2023-06-03              0.0  \n2023-06-04              0.0  \n\n[1963 rows x 38 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>log_close</th>\n      <th>Close</th>\n      <th>log_returns</th>\n      <th>diff_fear_greed</th>\n      <th>volatility_kcp</th>\n      <th>volume_em</th>\n      <th>volume_vpt</th>\n      <th>momentum_stoch_rsi</th>\n      <th>volatility_bbp</th>\n      <th>momentum_stoch</th>\n      <th>...</th>\n      <th>trend_macd_diff</th>\n      <th>trend_stc</th>\n      <th>volume_sma_em</th>\n      <th>trend_aroon_ind</th>\n      <th>momentum_tsi</th>\n      <th>trend_aroon_down</th>\n      <th>trend_vortex_ind_neg</th>\n      <th>trend_psar_down_indicator</th>\n      <th>trend_adx_neg</th>\n      <th>volatility_bbli</th>\n    </tr>\n    <tr>\n      <th>timestamp</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2018-01-19</th>\n      <td>6.946110</td>\n      <td>1039.099976</td>\n      <td>0.002718</td>\n      <td>-0.008588</td>\n      <td>0.102478</td>\n      <td>31.624126</td>\n      <td>1.400980e+08</td>\n      <td>0.037616</td>\n      <td>0.422813</td>\n      <td>39.600400</td>\n      <td>...</td>\n      <td>-36.805177</td>\n      <td>0.000000</td>\n      <td>-60.164463</td>\n      <td>56.0</td>\n      <td>21.811785</td>\n      <td>20.0</td>\n      <td>0.992330</td>\n      <td>0.0</td>\n      <td>26.633272</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2018-01-20</th>\n      <td>7.051986</td>\n      <td>1155.150024</td>\n      <td>0.105875</td>\n      <td>0.094570</td>\n      <td>0.454029</td>\n      <td>176.900699</td>\n      <td>4.550768e+08</td>\n      <td>0.209534</td>\n      <td>0.564169</td>\n      <td>57.400634</td>\n      <td>...</td>\n      <td>-32.297077</td>\n      <td>0.000000</td>\n      <td>-48.724324</td>\n      <td>56.0</td>\n      <td>20.500950</td>\n      <td>16.0</td>\n      <td>0.955632</td>\n      <td>0.0</td>\n      <td>24.958464</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2018-01-21</th>\n      <td>6.956145</td>\n      <td>1049.579956</td>\n      <td>-0.095840</td>\n      <td>-0.107145</td>\n      <td>0.189903</td>\n      <td>-69.272645</td>\n      <td>1.352358e+08</td>\n      <td>0.033644</td>\n      <td>0.361546</td>\n      <td>41.207862</td>\n      <td>...</td>\n      <td>-35.591281</td>\n      <td>0.000000</td>\n      <td>-63.621180</td>\n      <td>56.0</td>\n      <td>17.246075</td>\n      <td>12.0</td>\n      <td>0.996763</td>\n      <td>0.0</td>\n      <td>24.462554</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2018-01-22</th>\n      <td>6.911010</td>\n      <td>1003.260010</td>\n      <td>-0.045135</td>\n      <td>-0.056441</td>\n      <td>0.117475</td>\n      <td>-326.973658</td>\n      <td>-4.768732e+08</td>\n      <td>0.000000</td>\n      <td>0.255003</td>\n      <td>34.103119</td>\n      <td>...</td>\n      <td>-39.740806</td>\n      <td>0.000000</td>\n      <td>-96.179267</td>\n      <td>56.0</td>\n      <td>13.818018</td>\n      <td>8.0</td>\n      <td>1.069065</td>\n      <td>0.0</td>\n      <td>27.014842</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2018-01-23</th>\n      <td>6.893889</td>\n      <td>986.229004</td>\n      <td>-0.017121</td>\n      <td>-0.028427</td>\n      <td>0.169233</td>\n      <td>-109.817190</td>\n      <td>-2.285243e+08</td>\n      <td>0.000000</td>\n      <td>0.217811</td>\n      <td>31.490833</td>\n      <td>...</td>\n      <td>-42.249176</td>\n      <td>0.000000</td>\n      <td>-118.460975</td>\n      <td>56.0</td>\n      <td>10.820051</td>\n      <td>4.0</td>\n      <td>1.120754</td>\n      <td>0.0</td>\n      <td>26.135747</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2023-05-31</th>\n      <td>7.535900</td>\n      <td>1874.130493</td>\n      <td>-0.014249</td>\n      <td>-1.014249</td>\n      <td>0.765721</td>\n      <td>-18.994183</td>\n      <td>-6.215059e+07</td>\n      <td>0.627877</td>\n      <td>0.807676</td>\n      <td>67.930616</td>\n      <td>...</td>\n      <td>11.160775</td>\n      <td>94.726423</td>\n      <td>4.602985</td>\n      <td>60.0</td>\n      <td>2.887780</td>\n      <td>28.0</td>\n      <td>0.886572</td>\n      <td>0.0</td>\n      <td>18.486077</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2023-06-01</th>\n      <td>7.529515</td>\n      <td>1862.201416</td>\n      <td>-0.006385</td>\n      <td>1.993615</td>\n      <td>0.590639</td>\n      <td>-9.265335</td>\n      <td>-1.205696e+08</td>\n      <td>0.493067</td>\n      <td>0.701580</td>\n      <td>60.614706</td>\n      <td>...</td>\n      <td>8.930618</td>\n      <td>97.363212</td>\n      <td>4.538830</td>\n      <td>60.0</td>\n      <td>2.991828</td>\n      <td>24.0</td>\n      <td>0.908415</td>\n      <td>0.0</td>\n      <td>18.237269</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2023-06-02</th>\n      <td>7.553421</td>\n      <td>1907.256592</td>\n      <td>0.023907</td>\n      <td>-2.976093</td>\n      <td>0.989787</td>\n      <td>13.536856</td>\n      <td>1.116329e+08</td>\n      <td>0.865498</td>\n      <td>0.953481</td>\n      <td>88.246317</td>\n      <td>...</td>\n      <td>9.968121</td>\n      <td>98.681606</td>\n      <td>5.120619</td>\n      <td>60.0</td>\n      <td>4.968846</td>\n      <td>20.0</td>\n      <td>0.869446</td>\n      <td>0.0</td>\n      <td>16.780865</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2023-06-03</th>\n      <td>7.545608</td>\n      <td>1892.412476</td>\n      <td>-0.007813</td>\n      <td>0.992187</td>\n      <td>0.790237</td>\n      <td>10.775132</td>\n      <td>1.205078e+08</td>\n      <td>0.714422</td>\n      <td>0.818537</td>\n      <td>79.142660</td>\n      <td>...</td>\n      <td>9.153992</td>\n      <td>99.340803</td>\n      <td>5.549683</td>\n      <td>60.0</td>\n      <td>5.860393</td>\n      <td>16.0</td>\n      <td>0.876527</td>\n      <td>0.0</td>\n      <td>16.222529</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2023-06-04</th>\n      <td>7.550950</td>\n      <td>1902.548706</td>\n      <td>0.005342</td>\n      <td>-0.005963</td>\n      <td>0.806855</td>\n      <td>0.219065</td>\n      <td>-8.870213e+06</td>\n      <td>0.791131</td>\n      <td>0.843562</td>\n      <td>85.359047</td>\n      <td>...</td>\n      <td>8.795296</td>\n      <td>99.670401</td>\n      <td>5.836365</td>\n      <td>60.0</td>\n      <td>6.994869</td>\n      <td>12.0</td>\n      <td>0.873427</td>\n      <td>0.0</td>\n      <td>15.660544</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>1963 rows Ã— 38 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\n",
    "    '../datasets/prepared/eth/ta_corr_01_fd.csv',\n",
    "    parse_dates=True\n",
    ")\n",
    "target_col = 'log_returns'\n",
    "df = df.set_index(['timestamp'])\n",
    "df.index = pd.to_datetime(df.index)\n",
    "exclude_cols = ['log_close', 'Close']\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T18:55:26.292491200Z",
     "start_time": "2023-06-04T18:55:26.241297500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss_fn': <function RMSELoss at 0x000002007BA070D0>, 'batch_size': 4, 'feature_size': 36, 'decoder_size': 16, 'timestep': 5, 'horizon': 14, 'num_layers': 4, 'dropout': 0.1, 'nhead': 4, 'attn_type': 'fac_dense', 'learning_rate': 1e-05, 'weight_decay': 1e-06}\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = 'checkpoints/multioutput/best_checkpoints/fac_dense_timestep=5epoch=44-val_loss=0.569.ckpt'\n",
    "checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
    "\n",
    "ntest=42\n",
    "\n",
    "print(checkpoint[\"hyper_parameters\"])\n",
    "\n",
    "#loss_fn=checkpoint[\"hyper_parameters\"]['loss_fn']\n",
    "batch_size = checkpoint[\"hyper_parameters\"]['batch_size']\n",
    "#num_layers= checkpoint[\"hyper_parameters\"]['num_layers']\n",
    "dropout =  checkpoint[\"hyper_parameters\"]['dropout']\n",
    "#nhead= checkpoint[\"hyper_parameters\"]['nhead']\n",
    "#feature_size= checkpoint[\"hyper_parameters\"]['feature_size']\n",
    "learning_rate= checkpoint[\"hyper_parameters\"]['learning_rate']\n",
    "weight_decay=checkpoint[\"hyper_parameters\"]['weight_decay']\n",
    "\n",
    "timestep=checkpoint[\"hyper_parameters\"]['timestep']\n",
    "horizon=checkpoint[\"hyper_parameters\"]['horizon']\n",
    "\n",
    "attn_type = checkpoint[\"hyper_parameters\"]['attn_type']\n",
    "model_checkpoint_outputdir = 'multioutput/btc_eth'\n",
    "patience = 30\n",
    "n_epochs = 100"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T19:05:06.600494300Z",
     "start_time": "2023-06-04T19:05:06.567417400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape (1945, 5, 36) Y.shape (1945, 14)\n"
     ]
    }
   ],
   "source": [
    "Xtrain, Ytrain, Xtest, Ytest, XVal, YVal, scaler = make_dataset(df, target_col='log_returns',\n",
    "                                                                exclude_cols=exclude_cols, timestep=timestep, ntest=ntest, horizon=horizon)\n",
    "train_loader, val_loader, test_loader, test_loader_one = get_torch_data_loaders(\n",
    "    Xtrain, Ytrain, Xtest, Ytest, XVal, YVal, batch_size\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T18:59:19.020621600Z",
     "start_time": "2023-06-04T18:59:18.964510Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kw_batch,feature size:  4 36\n",
      "embed_dim: 36\n",
      "num_heads:  4\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import TQDMProgressBar\n",
    "\n",
    "model = TransAm.load_from_checkpoint(checkpoint_path, map_location=torch.device('cpu'), loss_fn=RMSELoss)\n",
    "\n",
    "\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", patience=patience, verbose=False, mode=\"min\")\n",
    "filename = f\"{attn_type}_timestep={timestep}\" + '{epoch}-{val_loss:.3f}'\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=f\"checkpoints/{model_checkpoint_outputdir}\",\n",
    "    filename=filename,\n",
    "    save_top_k=1,\n",
    "    monitor=\"val_loss\"\n",
    ")\n",
    "\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    callbacks=[TQDMProgressBar(refresh_rate=0), early_stop_callback,checkpoint_callback],\n",
    "    max_epochs=n_epochs,\n",
    "    logger=True,\n",
    ")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T19:15:55.460565400Z",
     "start_time": "2023-06-04T19:15:55.394903100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name                | Type                    | Params\n",
      "----------------------------------------------------------------\n",
      "0 | pos_encoder         | PositionalEncoding      | 0     \n",
      "1 | encoder_layer       | TransformerEncoderLayer | 155 K \n",
      "2 | transformer_encoder | TransformerEncoder      | 620 K \n",
      "3 | decoder1            | Linear                  | 2.5 K \n",
      "----------------------------------------------------------------\n",
      "777 K     Trainable params\n",
      "0         Non-trainable params\n",
      "777 K     Total params\n",
      "3.110     Total estimated model params size (MB)\n",
      "C:\\Users\\Olexandr\\miniconda3\\envs\\diploma\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning:\n",
      "\n",
      "The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\n",
      "C:\\Users\\Olexandr\\miniconda3\\envs\\diploma\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning:\n",
      "\n",
      "The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss tensor(1.8950, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.8402, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.6900, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.4584, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9509, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7283, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7362, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7093, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8297, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.0805, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.0539, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9810, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9173, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.0095, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.1601, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.1499, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.0915, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.0926, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.1170, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.2252, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.3125, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.2795, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.3080, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8814, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9711, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7336, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8577, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9052, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.0671, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.0430, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8998, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9837, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9989, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.0186, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.0820, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9513, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9259, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7273, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6960, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7138, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7570, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8106, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7258, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5967, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5866, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5677, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7715, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9652, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.1496, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.0916, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9538, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8085, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7897, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.2721, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.3447, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.5791, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.3221, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.2957, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.0948, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9825, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7076, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6609, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.0211, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.0589, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.0465, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6109, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.3093, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.2816, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.4082, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.4413, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7151, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.0553, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.4721, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.4744, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.5898, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.1304, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.1293, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9410, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9366, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.1281, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.4575, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.7868, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.6979, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.4765, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.2130, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.1817, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.1637, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.1860, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7954, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5462, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.4987, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6069, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8510, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8401, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.0288, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9353, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.1816, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.0180, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.0098, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6415, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5434, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.4138, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.3837, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.3883, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.4125, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8978, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9371, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9891, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7819, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6001, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5610, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5636, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5585, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6132, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5993, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.0146, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.3251, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.4211, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.3278, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9265, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6500, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7215, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7437, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7602, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6542, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7241, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8507, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.1266, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.1414, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.1653, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.0157, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.3966, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.4394, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.3606, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.0568, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6414, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5287, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6182, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7309, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8379, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8357, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7929, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7159, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5918, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5862, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.4775, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.4729, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5795, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7296, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.1021, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.1564, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.1635, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7626, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6241, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6224, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5649, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6416, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7879, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7666, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7851, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.3955, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.3029, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.3398, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5992, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7657, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8071, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7144, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.3811, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.3166, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.4391, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7812, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7775, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8320, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5627, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5270, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5666, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6840, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9199, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9207, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8799, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5579, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5392, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6508, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7222, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9736, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.0110, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.0445, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.0204, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9717, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8463, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.1095, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(2.9095, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(3.3684, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(3.4306, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(2.8552, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.2030, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9171, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8783, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.0949, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.0446, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.2653, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9863, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9255, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8859, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6898, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6120, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8275, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8096, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8555, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6926, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5850, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8386, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8172, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7730, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6981, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.4985, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5097, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.4805, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.4809, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.4682, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.4513, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.4519, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.4252, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.3453, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.4574, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6787, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8324, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9776, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8631, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8039, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7594, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6902, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8006, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6843, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8473, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.2899, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.4121, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.3748, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.1031, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8487, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9533, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8390, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7677, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.4944, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.4006, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.4098, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.4482, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5324, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5642, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5801, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5097, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7027, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7324, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7405, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6709, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8154, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.0128, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.1113, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.0935, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9369, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7777, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7517, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7304, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7892, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8345, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.2508, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.5492, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.6099, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.7233, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.1847, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.8192, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.6629, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.5392, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.3934, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.1878, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.1030, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7770, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6051, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9661, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.1254, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.2151, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.0469, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.0139, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9165, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7951, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7491, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6584, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7862, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8820, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8827, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8355, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8242, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8330, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8726, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9543, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.0618, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.2271, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.1743, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.4043, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.8697, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(2.4021, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(2.6192, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(2.6553, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.8585, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.3128, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.1710, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8720, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.0594, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.2814, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.3969, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.3166, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.0631, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9616, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9181, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8328, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8515, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7536, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7420, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6357, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7601, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8306, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9799, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9170, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7732, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7438, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6911, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8667, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.0724, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.1291, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.0373, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9118, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.1886, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.1500, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.2387, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9378, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7609, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5772, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5120, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6663, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6655, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8216, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7599, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7034, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5606, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.4759, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7447, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8019, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9226, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8947, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7690, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7639, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8492, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9374, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8340, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7255, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5190, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5506, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6754, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7479, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7806, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6925, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9947, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.1029, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.1057, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8763, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7864, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8101, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8683, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8805, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8024, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9057, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8852, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9422, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9743, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7937, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6980, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5618, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.4991, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.4896, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.4684, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6031, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6862, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7106, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6097, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.4969, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5360, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6084, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7298, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9699, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.1595, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.2620, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.1285, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9406, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9202, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9078, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8931, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8982, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.2937, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.5961, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.7094, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.5013, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.1494, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8500, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6831, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8035, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.0634, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.3022, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.3029, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.3519, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.0390, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9981, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8094, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7757, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7806, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9738, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9727, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.1169, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9033, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9114, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6265, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8196, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.0289, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.0829, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.0230, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6823, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.4960, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.3620, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.3761, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.3204, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.3375, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5018, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7206, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7743, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8488, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.5506, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.7167, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.6769, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.3638, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6131, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6027, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6189, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5433, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5643, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5826, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6176, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5651, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5288, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.3539, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.2494, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.2813, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.4031, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5336, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6146, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6206, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6307, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6231, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5266, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6010, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5438, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6481, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5636, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5578, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.4257, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.4368, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5097, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.9289, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.8524, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.6653, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.4118, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9331, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7452, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7373, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7097, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8422, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.0864, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.0725, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9493, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9354, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.0148, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.1885, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.1476, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.0678, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.0668, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.0903, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.1937, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.2933, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.2605, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.2761, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8966, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9799, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7381, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8661, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9245, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.1004, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.0485, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8689, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.0020, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.0030, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.0398, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.0952, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9147, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9407, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7138, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7001, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6947, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7691, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7941, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7176, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5861, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5596, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5817, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7770, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.0098, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.1539, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.0821, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9592, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8132, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7714, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.2756, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.3509, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.5756, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.3367, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.2829, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.1364, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9533, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7051, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6447, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.0405, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.0987, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.0617, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6091, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.2587, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.2857, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.3990, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.3928, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7272, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.0693, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.5002, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.4864, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.5697, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.1597, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.1407, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9328, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9842, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.1014, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.4434, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.7587, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.6885, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.4468, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.2469, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.1941, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.1620, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.1617, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7769, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5498, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5027, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5984, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8605, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7946, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.0393, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9208, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.1678, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.0371, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.0750, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6248, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5274, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.4234, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.3521, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.3731, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.3991, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8631, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9479, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9985, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7838, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6089, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5378, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5874, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5501, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5756, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5889, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9925, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.3411, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.4174, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.3078, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9234, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6870, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7160, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7279, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7654, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6255, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7487, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8386, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.1203, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.1104, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.1354, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.0016, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.3946, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.4404, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.3543, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.0370, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6422, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5033, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5619, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7262, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8405, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8304, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8195, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7339, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6052, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5860, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.4756, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.4628, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5811, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7280, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.1374, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.1857, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.1553, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7275, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6191, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5968, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5542, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6280, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7727, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7542, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7520, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.3746, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.3217, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.3435, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6123, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7860, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7878, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7020, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.4101, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.3037, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.4382, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8001, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7946, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8049, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5877, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5151, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5961, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6741, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9007, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8946, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8875, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5800, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5477, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6591, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7011, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9810, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.0208, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.0570, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.0104, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9386, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8192, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.1140, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(2.9214, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(3.3671, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(3.4366, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(2.8314, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.1966, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9148, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8669, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.0937, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.0590, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.2612, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9962, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9395, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9088, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6862, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6158, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8741, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7987, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8352, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6811, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5919, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8144, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7951, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7842, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6943, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.4964, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.4973, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.4855, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.4909, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.4564, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.4499, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.4622, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.4488, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.3361, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.4713, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6902, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8148, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9652, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8536, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7974, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7463, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6900, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7458, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6960, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8515, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.2841, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.3847, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.3840, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.1219, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7857, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9064, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8543, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7632, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5036, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.4181, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.4322, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.4606, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5344, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5692, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5770, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5168, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7133, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7000, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7292, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6866, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8208, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.0028, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.1354, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.1370, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9402, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7989, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7609, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7450, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7811, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8609, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.2353, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.5221, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.6075, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.7578, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.1577, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.7811, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.6314, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.5592, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.3844, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.1621, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.0565, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7901, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5709, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9428, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.1283, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.2077, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.0925, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9780, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8936, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7377, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7328, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6694, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7514, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8581, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8871, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8154, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8258, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8023, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8651, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9351, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.0659, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.2319, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.2135, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.4343, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.8663, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(2.4085, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(2.5638, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(2.6153, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.7910, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.1743, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.1858, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9171, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.0528, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.2925, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.3448, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.3184, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.0389, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9732, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8881, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8403, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8437, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7868, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7284, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6620, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7424, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8650, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9836, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9391, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7816, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7543, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6799, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8517, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.0482, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.1238, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.0362, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9054, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.1413, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.1636, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.2570, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9540, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7333, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5783, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5094, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6698, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6758, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7945, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7663, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7275, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5775, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.4795, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7440, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8469, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9215, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8941, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7389, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7675, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8529, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.9253, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8156, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7191, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5195, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.5382, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.6690, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7520, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7975, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7375, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.0014, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.1166, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(1.1208, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8925, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.8333, grad_fn=<SqrtBackward0>)\n",
      "training_loss tensor(0.7968, grad_fn=<SqrtBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Olexandr\\miniconda3\\envs\\diploma\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:48: UserWarning:\n",
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, train_loader, val_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T19:11:56.521784Z",
     "start_time": "2023-06-04T19:11:22.083453400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
